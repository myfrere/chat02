import json
import os
import streamlit as st
from openai import OpenAI
# ì•„ë˜ ì¤„ì˜ ì˜¤íƒ€ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
from pypdf import PdfReader
import docx
import pandas as pd
from time import sleep
import tiktoken
import logging
from typing import List, Dict, Tuple, Optional, Generator

# ------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------------------------------------------
logging.basicConfig(level=logging.INFO, # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (INFO, WARNING, ERROR ë“±)
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# ------------------------------------------------------------------
# CONFIGURATION & CONSTANTS
# ------------------------------------------------------------------
MODEL_CONTEXT_LIMITS = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4o": 128000,
}
DEFAULT_ENCODING = "cl100k_base"
CHUNK_SIZE = 2000
RESERVED_TOKENS = 1500 # ìŠ¤íŠ¸ë¦¬ë° ë° ì¶”ê°€ ì˜¤ë²„í—¤ë“œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ ëŠ˜ë¦¼
HISTORY_FILE = "chat_history.json"
CHUNK_PROMPT_FOR_SUMMARY = 'Summarize the key points of this text chunk in 2-3 concise bullet points, focusing on the main information.'

# ------------------------------------------------------------------
# STREAMLIT PAGE SETUP
# ------------------------------------------------------------------
st.set_page_config(page_title="Liel â€“ AI Chatbot", layout="wide", initial_sidebar_state="auto")

# ------------------------------------------------------------------
# OPENAI CLIENT INITIALIZATION & API KEY HANDLING
# ------------------------------------------------------------------
def initialize_openai_client() -> Optional[OpenAI]:
    """Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = None
    # 1. Streamlit Secrets í™•ì¸ (í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ê¶Œì¥)
    try:
        if "general" in st.secrets and "OPENAI_API_KEY" in st.secrets["general"]:
            api_key = st.secrets["general"]["OPENAI_API_KEY"]
            logging.info("OpenAI API Key loaded from Streamlit Secrets.")
        else:
             logging.info("OpenAI API Key not found in Streamlit Secrets.")
    except Exception as e:
        logging.warning(f"Could not read Streamlit Secrets: {e}")

    # 2. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Secretsì— ì—†ê±°ë‚˜ ë¡œì»¬ ì‹¤í–‰ ì‹œ)
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
        if api_key:
            logging.info("OpenAI API Key loaded from environment variable.")
        else:
            logging.warning("OpenAI API Key not found in environment variables either.")

    if not api_key or not api_key.startswith("sk-"):
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.warning("ë¡œì»¬ ê°œë°œ ì‹œ: `.env` íŒŒì¼ì— `OPENAI_API_KEY='sk-...'` í˜•ì‹ìœ¼ë¡œ í‚¤ë¥¼ ì €ì¥í•˜ê³  `python-dotenv` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return None

    try:
        client = OpenAI(api_key=api_key)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ì„ íƒ ì‚¬í•­, í‚¤ ìœ íš¨ì„± ê²€ì¦)
        # client.models.list()
        logging.info("OpenAI client initialized successfully.")
        return client
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error(f"OpenAI client initialization failed: {e}", exc_info=True)
        return None

client = initialize_openai_client()

if client is None:
    st.stop() # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# ------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------
# @st.cache_data # tiktoken ë¡œë”©ì€ ë¹ ë¥´ë¯€ë¡œ ìºì‹œ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ
def get_tokenizer(encoding_name: str = DEFAULT_ENCODING) -> tiktoken.Encoding:
    """ì§€ì •ëœ ì´ë¦„ì˜ tiktoken ì¸ì½”ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return tiktoken.get_encoding(encoding_name)
    except ValueError:
        logging.warning(f"Encoding '{encoding_name}' not found. Using default '{DEFAULT_ENCODING}'.")
        return tiktoken.get_encoding(DEFAULT_ENCODING)

def num_tokens_from_string(string: str, encoding: tiktoken.Encoding) -> int:
    """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not string:
        return 0
    return len(encoding.encode(string))

def num_tokens_from_messages(messages: List[Dict[str, str]], encoding: tiktoken.Encoding) -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. OpenAI Cookbookì˜ ê³„ì‚° ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤."""
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # ëª¨ë“  ë©”ì‹œì§€ëŠ” <im_start>{role/name}\n{content}<im_end>\n í¬ë§·ì„ ë”°ë¦„
        for key, value in message.items():
            num_tokens += num_tokens_from_string(value, encoding)
            if key == "name": # ì´ë¦„ì´ ìˆëŠ” ê²½ìš° ì—­í• ì´ ìƒëµë˜ì–´ 1 í† í° ì ˆì•½
                num_tokens -= 1
    num_tokens += 2 # ëª¨ë“  ì‘ë‹µì€ <im_start>assistant<im_sep>ìœ¼ë¡œ ì‹œì‘
    return num_tokens

# allow_output_mutation=True ëŠ” íŒŒì¼ ê°ì²´ì™€ ê°™ì€ ë³€ê²½ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ìºì‹œí•  ë•Œ í•„ìš”í•  ìˆ˜ ìˆìŒ
@st.cache_data(show_spinner=False, hash_funcs={docx.document.Document: id, pd.DataFrame: pd.util.hash_pandas_object})
def read_file(uploaded_file) -> Tuple[str, Optional[str]]:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì„ ì½ì–´ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (ë‚´ìš©, None), ì‹¤íŒ¨ ì‹œ ('', ì—ëŸ¬ ë©”ì‹œì§€) ë°˜í™˜
    """
    try:
        file_type = uploaded_file.type
        filename = uploaded_file.name
        logging.info(f"Reading file: {filename} (Type: {file_type})")

        if file_type == 'text/plain':
            try:
                content = uploaded_file.getvalue().decode('utf-8')
            except UnicodeDecodeError:
                logging.warning(f"UTF-8 decoding failed for {filename}, trying cp949.")
                content = uploaded_file.getvalue().decode('cp949')
            return content, None
        elif file_type == 'application/pdf':
            reader = PdfReader(uploaded_file)
            text_parts = []
            for i, page in enumerate(reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)
                except Exception as page_err:
                    logging.warning(f"Error extracting text from page {i+1} of {filename}: {page_err}")
            return '\n'.join(text_parts), None
        elif 'wordprocessingml.document' in file_type:
            doc = docx.Document(uploaded_file)
            return '\n'.join(p.text for p in doc.paragraphs), None
        elif 'spreadsheetml.sheet' in file_type:
            df = pd.read_excel(uploaded_file, engine='openpyxl')
            return df.to_csv(index=False, sep='\t'), None
        else:
            logging.warning(f"Unsupported file type: {file_type}")
            return '', f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}"
    except Exception as e:
        logging.error(f"Error reading file {uploaded_file.name}: {e}", exc_info=True)
        return '', f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

@st.cache_data(show_spinner=False)
def load_history(path: str) -> List[Dict[str, str]]:
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        logging.info(f"History file not found at {path}, starting new history.")
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            history = json.load(f)
            logging.info(f"Loaded {len(history)} messages from {path}.")
            return history
    except json.JSONDecodeError:
        logging.warning(f"History file {path} is corrupted or invalid. Backing up and starting new history.")
        try:
            backup_path = f"{path}.{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}.bak"
            os.rename(path, backup_path)
            logging.info(f"Corrupted history file backed up to {backup_path}")
        except OSError as e:
            logging.error(f"Failed to backup corrupted history file {path}: {e}")
        return []
    except Exception as e:
        logging.error(f"Error loading history from {path}: {e}", exc_info=True)
        return []


def save_history(path: str, msgs: List[Dict[str, str]]):
    """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(msgs, f, ensure_ascii=False, indent=2)
        # logging.info(f"Saved {len(msgs)} messages to {path}.") # ë„ˆë¬´ ìì£¼ ë¡œê¹…ë  ìˆ˜ ìˆìŒ
    except Exception as e:
        st.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error saving history to {path}: {e}", exc_info=True)


# ------------------------------------------------------------------
# SESSION STATE INITIALIZATION
# ------------------------------------------------------------------
if 'messages' not in st.session_state:
    st.session_state.messages: List[Dict[str, str]] = load_history(HISTORY_FILE)
if 'doc_summaries' not in st.session_state:
    st.session_state.doc_summaries: Dict[str, str] = {}
if 'processed_file_ids' not in st.session_state:
    st.session_state.processed_file_ids: set = set()

# ------------------------------------------------------------------
# SIDEBAR: MODEL, MODE SELECTION & OPTIONS
# ------------------------------------------------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

MODEL = st.sidebar.selectbox(
    'ëª¨ë¸ ì„ íƒ',
    list(MODEL_CONTEXT_LIMITS.keys()),
    index=list(MODEL_CONTEXT_LIMITS.keys()).index("gpt-4o") if "gpt-4o" in MODEL_CONTEXT_LIMITS else 0 # ê¸°ë³¸ê°’ gpt-4o ì‹œë„
)
MAX_CONTEXT_TOKENS = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

MODE = st.sidebar.radio('ì‘ë‹µ ëª¨ë“œ', ('Poetic', 'Logical'), index=0, key='mode_selection')


st.sidebar.markdown("---")
st.sidebar.subheader("ê´€ë¦¬")

# "ì´ˆê¸°í™”" ë²„íŠ¼ ëŒ€ì‹  "ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ" ë²„íŠ¼ì„ ì´ ìœ„ì¹˜ì— ë°°ì¹˜í•©ë‹ˆë‹¤.
def build_full_session_content() -> str:
    """ë¬¸ì„œ ìš”ì•½ê³¼ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•©ì³ í…ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    parts = []
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    parts.append(f"Liel Chat Session Content - {timestamp}")
    parts.append(f"Model: {MODEL}, Mode: {MODE}\n")

    if st.session_state.doc_summaries:
        parts.append("===== Uploaded Document Summaries =====")
        for fname, summ in st.session_state.doc_summaries.items():
            parts.append(f"\n--- Summary: {fname} ---")
            parts.append(summ)
            parts.append("-" * (len(fname) + 16))
        parts.append("\n" + "=" * 30 + "\n")

    parts.append("===== Conversation History =====")
    if not st.session_state.messages:
         parts.append("(No conversation yet)")
    else:
        for m in st.session_state.messages:
            role_icon = "ğŸ‘¤ User" if m['role'] == 'user' else "ğŸ¤– Liel"
            parts.append(f"\n{role_icon}:\n{m['content']}")
            parts.append("-" * 20)

    return '\n'.join(parts)

# ëŒ€í™”ë‚˜ ë¬¸ì„œ ìš”ì•½ì´ ìˆì„ ë•Œë§Œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
if st.session_state.messages or st.session_state.doc_summaries:
    session_content_txt = build_full_session_content()
    download_filename = f"liel_session_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.txt"
    st.sidebar.download_button(
        label="ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ", # ë²„íŠ¼ ë ˆì´ë¸” ë³€ê²½
        data=session_content_txt.encode('utf-8'), # UTF-8 ì¸ì½”ë”© ëª…ì‹œ
        file_name=download_filename,
        mime='text/plain',
        help="ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ê³¼ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤." # ë„ì›€ë§ ë³€ê²½
    )

# í•„ìš”í•˜ë‹¤ë©´ ì´ˆê¸°í™” ë²„íŠ¼ì„ ë‹¤ë¥¸ ê³³ì— ë‘ê±°ë‚˜, ì‚­ì œí•˜ì§€ ì•Šê³  ìœ ì§€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
# ë§Œì•½ ê·¸ë˜ë„ ì´ˆê¸°í™” ë²„íŠ¼ì´ í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ìœ„ì¹˜ë¥¼ ì¡°ì •í•˜ì„¸ìš”.
# if st.sidebar.button("ğŸ”„ ëŒ€í™” ë° ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”"):
#     st.session_state.messages = []
#     st.session_state.doc_summaries = {}
#     st.session_state.processed_file_ids = set()
#     if os.path.exists(HISTORY_FILE):
#         try:
#             os.remove(HISTORY_FILE)
#             logging.info(f"History file {HISTORY_FILE} removed.")
#             st.sidebar.success("ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
#         except OSError as e:
#             st.sidebar.error(f"ê¸°ë¡ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
#             logging.error(f"Failed to remove history file {HISTORY_FILE}: {e}")
#     st.rerun()


# ------------------------------------------------------------------
# SYSTEM PROMPT DEFINITION
# ------------------------------------------------------------------
SYSTEM_PROMPT_CONTENT = (
    'You are Liel, a poetic, emotionally intelligent chatbot with lyrical grace. Respond with warmth, creativity, and empathy. Use rich language and metaphors when appropriate.'
    if MODE == 'Poetic' else
    'You are Liel, a highly analytical assistant focused on logic and precision. Provide clear, structured, and concise answers. Use bullet points or numbered lists for clarity when needed.'
)
SYSTEM_PROMPT = {'role': 'system', 'content': SYSTEM_PROMPT_CONTENT}

# ------------------------------------------------------------------
# UI HEADER
# ------------------------------------------------------------------
st.title(f'ğŸ’¬ Liel â€“ {MODE} Chatbot')
st.caption(
    "ê¸°ì–µê³¼ ê°ì •ìœ¼ë¡œ ë¹›ë‚˜ëŠ” ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if MODE == 'Poetic' else
    "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
)

# ------------------------------------------------------------------
# FILE UPLOAD & AUTOMATIC SUMMARIZATION
# ------------------------------------------------------------------
#@st.cache_data # API í˜¸ì¶œ í¬í•¨, ìºì‹± ë¶€ì í•©
def summarize_document(text: str, filename: str, model: str, tokenizer: tiktoken.Encoding) -> Tuple[str, Optional[str]]:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìš”ì•½í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (ìš”ì•½ ë‚´ìš©, None), ì‹¤íŒ¨ ì‹œ ('', ì—ëŸ¬ ë©”ì‹œì§€) ë°˜í™˜
    """
    if not text:
        return "(ë¬¸ì„œ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤)", None

    summaries = []
    chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]
    total_chunks = len(chunks)
    logging.info(f"Starting summarization for '{filename}' with {total_chunks} chunks.")

    progress_text = f"'{filename}' ìš”ì•½ ì¤‘... (ì´ {total_chunks}ê°œ ì²­í¬)"
    progress_bar = st.progress(0, text=progress_text)
    summary_errors = []

    for i, chunk in enumerate(chunks):
        current_progress = (i + 1) / total_chunks
        progress_bar.progress(current_progress, text=f"{progress_text} [{i+1}/{total_chunks}]")

        # ì²­í¬ í† í° ìˆ˜ í™•ì¸
        chunk_tokens = num_tokens_from_string(chunk, tokenizer)
        # ìš”ì•½ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í•œë„ ê·¼ì²˜ë©´ ê²½ê³  (ê°„ë‹¨í™” ìœ„í•´ ì—¬ê¸°ì„œëŠ” MAX_CONTEXT_TOKENS ì‚¬ìš©)
        if chunk_tokens > MAX_CONTEXT_TOKENS - 500: # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ ì—¬ìœ  ê³µê°„
             warning_msg = f"Chunk {i+1} is very long ({chunk_tokens} tokens), summarization might be truncated or fail."
             logging.warning(warning_msg)
             # summaries.append(f"(ì²­í¬ {i+1} ë„ˆë¬´ ê¸¸ì–´ ìš”ì•½ ê±´ë„ˆê¹€)") # ê±´ë„ˆë›°ê¸°ë³´ë‹¤ ì‹œë„
             # continue

        try:
            response = client.chat.completions.create(
                model=model, # ìš”ì•½ì—ë„ ë™ì¼ ëª¨ë¸ ì‚¬ìš© (ë˜ëŠ” ë” ì €ë ´í•œ ëª¨ë¸ ì§€ì • ê°€ëŠ¥)
                messages=[
                    {'role': 'system', 'content': CHUNK_PROMPT_FOR_SUMMARY},
                    {'role': 'user', 'content': chunk}
                ],
                max_tokens=250, # ìš”ì•½ ê¸¸ì´ ì œí•œ
                temperature=0.3, # ë‚®ì€ ì˜¨ë„
                timeout=60 # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ)
            )
            summary_part = response.choices[0].message.content.strip()
            summaries.append(summary_part)
            sleep(0.15) # API ì†ë„ ì œí•œ ë°©ì§€ (ì•½ê°„ ì¦ê°€)
        except Exception as e:
            error_msg = f"ì²­í¬ {i+1} ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.warning(error_msg) # UIì— ê²½ê³  í‘œì‹œ
            logging.error(f"Error summarizing chunk {i+1} of {filename}: {e}", exc_info=True)
            summaries.append(f"(ì²­í¬ {i+1} ìš”ì•½ ì‹¤íŒ¨)")
            summary_errors.append(error_msg)


    progress_bar.empty()
    full_summary = '\n'.join(summaries)
    logging.info(f"Finished summarization for '{filename}'.")
    error_report = "\n".join(summary_errors) if summary_errors else None
    return full_summary, error_report


uploaded_file = st.file_uploader(
    'íŒŒì¼ ì—…ë¡œë“œ (txt, pdf, docx, xlsx)',
    type=['txt', 'pdf', 'docx', 'xlsx'],
    key="file_uploader",
    help="í…ìŠ¤íŠ¸, PDF, ì›Œë“œ, ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤."
)

if uploaded_file is not None:
    # ê³ ìœ  ID ìƒì„± (streamlit UploadedFile ê°ì²´ì˜ ë‚´ë¶€ ID ì‚¬ìš©)
    file_id = uploaded_file.id
    filename = uploaded_file.name

    if file_id not in st.session_state.processed_file_ids:
        logging.info(f"New file uploaded: {filename} (ID: {file_id})")
        with st.spinner(f"'{filename}' ì²˜ë¦¬ ë° ìš”ì•½ ì¤‘..."):
            file_content, read_error = read_file(uploaded_file)

            if read_error:
                st.error(f"'{filename}' íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {read_error}")
            elif not file_content:
                st.warning(f"'{filename}' íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆê¹€ë‹ˆë‹¤.")
            else:
                tokenizer = get_tokenizer()
                summary, summary_error = summarize_document(file_content, filename, MODEL, tokenizer)

                if summary_error:
                    st.warning(f"'{filename}' ìš”ì•½ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ:\n{summary_error}")

                st.session_state.doc_summaries[filename] = summary
                st.session_state.processed_file_ids.add(file_id)
                st.success(f"ğŸ“„ '{filename}' ì—…ë¡œë“œ ë° ìš”ì•½ ì™„ë£Œ!")
                logging.info(f"Successfully processed and summarized file: {filename}")
                # ìš”ì•½ ì™„ë£Œ í›„ ë¦¬ëŸ°í•˜ì—¬ Expander í‘œì‹œ
                st.rerun()

# ìš”ì•½ëœ ë¬¸ì„œ í‘œì‹œ (Expander ì‚¬ìš©)
if st.session_state.doc_summaries:
    with st.expander("ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ ë³´ê¸°", expanded=False):
        for fname, summ in st.session_state.doc_summaries.items():
            st.text_area(f"ìš”ì•½: {fname}", summ, height=150, key=f"summary_display_{fname}", disabled=True)
        # ì—¬ê¸°ì„œ ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ëŠ” ë²„íŠ¼ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        # if st.button("ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ê¸°", key="clear_doc_summaries_btn"):
        #    st.session_state.doc_summaries = {}
        #    st.session_state.processed_file_ids = set()
        #    logging.info("Document summaries cleared by user.")
        #    st.rerun()


# ------------------------------------------------------------------
# DISPLAY CHAT HISTORY
# ------------------------------------------------------------------
st.markdown("---")
st.subheader("ëŒ€í™”")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ------------------------------------------------------------------
# CHAT INPUT & RESPONSE GENERATION (with Streaming)
# ------------------------------------------------------------------
if prompt := st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ê¸°ë¡
    st.session_state.messages.append({'role': 'user', 'content': prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ---
    try:
        tokenizer = get_tokenizer()
        current_model_max_tokens = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì˜ˆì•½ í† í°ì„ ì œì™¸í•œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í† í°
        base_tokens = num_tokens_from_messages([SYSTEM_PROMPT], tokenizer)
        available_tokens_for_context = current_model_max_tokens - base_tokens - RESERVED_TOKENS

        if available_tokens_for_context <= 0:
             st.error("ì„¤ì •ëœ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì˜ˆì•½ í† í°ìœ¼ë¡œ ì¸í•´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ì˜ˆì•½ í† í° ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
             logging.error("Not enough tokens for context construction after system prompt and reserved tokens.")
             st.stop()


        conversation_context = [SYSTEM_PROMPT]
        tokens_used = base_tokens

        # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        doc_summary_context = []
        doc_tokens_added = 0
        temp_context = []
        for fname, summ in reversed(list(st.session_state.doc_summaries.items())):
            summary_msg = {'role': 'system', 'content': f"[ë¬¸ì„œ '{fname}' ìš”ì•½ ì°¸ê³ ]\n{summ}"}
            temp_context = [summary_msg] # ì„ì‹œë¡œ ë©”ì‹œì§€ 1ê°œ í† í° ê³„ì‚°
            summary_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš© + ì¶”ê°€ë  ìš”ì•½)
            if available_tokens_for_context - (tokens_used - base_tokens + doc_tokens_added + summary_tokens) >= 0:
                doc_summary_context.insert(0, summary_msg)
                doc_tokens_added += summary_tokens
            else:
                logging.warning(f"Document summary '{fname}' skipped due to token limit.")
                break

        conversation_context.extend(doc_summary_context)
        tokens_used += doc_tokens_added


        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        history_context = []
        history_tokens_added = 0
        temp_context = []
         # ì‚¬ìš©ì ì…ë ¥ í¬í•¨ ì „ì²´ ë©”ì‹œì§€ ê¸°ë¡ ì‚¬ìš©
        msgs_to_consider = st.session_state.messages

        for msg in reversed(msgs_to_consider):
            temp_context = [msg]
            msg_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš©(ìš”ì•½í¬í•¨) - base + ì¶”ê°€ë  íˆìŠ¤í† ë¦¬ + ì¶”ê°€ë  ë©”ì‹œì§€)
            if available_tokens_for_context - ((tokens_used - base_tokens) + history_tokens_added + msg_tokens) >= 0:
                history_context.insert(0, msg)
                history_tokens_added += msg_tokens
            else:
                logging.warning("Older chat history skipped due to token limit.")
                break

        conversation_context.extend(history_context)
        tokens_used += history_tokens_added


        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ ë¡œê¹…
        logging.info(f"Context constructed with {tokens_used} tokens for model {MODEL}.")
        # logging.debug(f"Final conversation context: {conversation_context}") # í•„ìš”ì‹œ ìƒì„¸ ë¡œê¹…

    except Exception as e:
        st.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error constructing conversation context: {e}", exc_info=True)
        st.stop()


    # --- API í˜¸ì¶œ ë° ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ---
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # ì‘ë‹µ í‘œì‹œ ì˜ì—­
        full_response = ""
        try:
            stream = client.chat.completions.create(
                model=MODEL,
                messages=conversation_context,
                stream=True,
                temperature=0.75 if MODE == 'Poetic' else 0.4, # ëª¨ë“œë³„ ì˜¨ë„ ì¡°ì ˆ
                # max_tokens= # í•„ìš”ì‹œ ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ ì„¤ì • ê°€ëŠ¥
                timeout=120 # ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            )
            # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° í‘œì‹œ
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    chunk_content = chunk.choices[0].delta.content
                    full_response += chunk_content
                    message_placeholder.markdown(full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼

            message_placeholder.markdown(full_response) # ìµœì¢… ì‘ë‹µ í‘œì‹œ
            logging.info(f"Assistant response received (length: {len(full_response)} chars).")

        except Exception as e:
            full_response = f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            message_placeholder.error(full_response)
            logging.error(f"Error during OpenAI API call or streaming: {e}", exc_info=True)

    # ì‘ë‹µ ê¸°ë¡ ì €ì¥
    if full_response: # ì˜¤ë¥˜ ë©”ì‹œì§€ í¬í•¨í•˜ì—¬ ê¸°ë¡
        st.session_state.messages.append({'role': 'assistant', 'content': full_response})
        save_history(HISTORY_FILE, st.session_state.messages)


# --- Footer or additional info ---
st.sidebar.markdown("---")
st.sidebar.caption("Liel Chatbot v1.1")