import json
import os
import streamlit as st
from openai import OpenAI
from pypdf import PdfReader
import docx
import pandas as pd
from time import sleep
import tiktoken
import logging
from typing import List, Dict, Tuple, Optional, Generator
import io # BytesIOë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€

# ------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------------------------------------------
logging.basicConfig(level=logging.INFO, # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (INFO, WARNING, ERROR ë“±)
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# ------------------------------------------------------------------
# CONFIGURATION & CONSTANTS
# ------------------------------------------------------------------
MODEL_CONTEXT_LIMITS = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4o": 128000,
}
DEFAULT_ENCODING = "cl100k_base"
CHUNK_SIZE = 2000
RESERVED_TOKENS = 1500 # ìŠ¤íŠ¸ë¦¬ë° ë° ì¶”ê°€ ì˜¤ë²„í—¤ë“œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ ëŠ˜ë¦¼
HISTORY_FILE = "chat_history.json"
CHUNK_PROMPT_FOR_SUMMARY = 'Summarize the key points of this text chunk in 2-3 concise bullet points, focusing on the main information.'

# ------------------------------------------------------------------
# STREAMLIT PAGE SETUP
# ------------------------------------------------------------------
st.set_page_config(page_title="Liel â€“ AI Chatbot", layout="wide", initial_sidebar_state="auto")

# ------------------------------------------------------------------
# OPENAI CLIENT INITIALIZATION & API KEY HANDLING
# ------------------------------------------------------------------
def initialize_openai_client() -> Optional[OpenAI]:
    """Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = None
    # 1. Streamlit Secrets í™•ì¸ (í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ê¶Œì¥)
    try:
        if "general" in st.secrets and "OPENAI_API_KEY" in st.secrets["general"]:
            api_key = st.secrets["general"]["OPENAI_API_KEY"]
            logging.info("OpenAI API Key loaded from Streamlit Secrets.")
        else:
             logging.info("OpenAI API Key not found in Streamlit Secrets.")
    except Exception as e:
        logging.warning(f"Could not read Streamlit Secrets: {e}")

    # 2. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Secretsì— ì—†ê±°ë‚˜ ë¡œì»¬ ì‹¤í–‰ ì‹œ)
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
        if api_key:
            logging.info("OpenAI API Key loaded from environment variable.")
        else:
            logging.warning("OpenAI API Key not found in environment variables either.")

    if not api_key or not api_key.startswith("sk-"):
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.warning("ë¡œì»¬ ê°œë°œ ì‹œ: `.env` íŒŒì¼ì— `OPENAI_API_KEY='sk-...'` í˜•ì‹ìœ¼ë¡œ í‚¤ë¥¼ ì €ì¥í•˜ê³  `python-dotenv` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return None

    try:
        client = OpenAI(api_key=api_key)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ì„ íƒ ì‚¬í•­, í‚¤ ìœ íš¨ì„± ê²€ì¦)
        # client.models.list()
        logging.info("OpenAI client initialized successfully.")
        return client
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error(f"OpenAI client initialization failed: {e}", exc_info=True)
        return None

client = initialize_openai_client()

if client is None:
    st.stop() # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# ------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------
# @st.cache_data # tiktoken ë¡œë”©ì€ ë¹ ë¥´ë¯€ë¡œ ìºì‹œ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ
def get_tokenizer(encoding_name: str = DEFAULT_ENCODING) -> tiktoken.Encoding:
    """ì§€ì •ëœ ì´ë¦„ì˜ tiktoken ì¸ì½”ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return tiktoken.get_encoding(encoding_name)
    except ValueError:
        logging.warning(f"Encoding '{encoding_name}' not found. Using default '{DEFAULT_ENCODING}'.")
        return tiktoken.get_encoding(DEFAULT_ENCODING)

def num_tokens_from_string(string: str, encoding: tiktoken.Encoding) -> int:
    """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not string:
        return 0
    return len(encoding.encode(string))

def num_tokens_from_messages(messages: List[Dict[str, str]], encoding: tiktoken.Encoding) -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. OpenAI Cookbookì˜ ê³„ì‚° ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤."""
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # ëª¨ë“  ë©”ì‹œì§€ëŠ” <im_start>{role/name}\n{content}<im_end>\n í¬ë§·ì„ ë”°ë¦„
        for key, value in message.items():
            num_tokens += num_tokens_from_string(value, encoding)
            if key == "name": # ì´ë¦„ì´ ìˆëŠ” ê²½ìš° ì—­í• ì´ ìƒëµë˜ì–´ 1 í† í° ì ˆì•½
                num_tokens -= 1
    num_tokens += 2 # ëª¨ë“  ì‘ë‹µì€ <im_start>assistant<im_sep>ìœ¼ë¡œ ì‹œì‘
    return num_tokens

# allow_output_mutation=True ëŠ” íŒŒì¼ ê°ì²´ì™€ ê°™ì€ ë³€ê²½ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ìºì‹œí•  ë•Œ í•„ìš”í•  ìˆ˜ ìˆìŒ
# ì´ í•¨ìˆ˜ëŠ” ì´ì œ uploaded_file ê°ì²´ ìì²´ê°€ ì•„ë‹Œ, ë°”ì´íŠ¸ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë°›ìŠµë‹ˆë‹¤.
@st.cache_data(show_spinner=False, hash_funcs={docx.document.Document: id, pd.DataFrame: pd.util.hash_pandas_object})
def read_file(uploaded_file_content_bytes, filename, file_type) -> Tuple[str, Optional[str]]:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ (bytes) ë°›ì•„ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (ë‚´ìš©, None), ì‹¤íŒ¨ ì‹œ ('', ì—ëŸ¬ ë©”ì‹œì§€) ë°˜í™˜
    """
    try:
        logging.info(f"Reading file content for: {filename} (Type: {file_type})")
        # BytesIOë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ë¥˜ ê°ì²´ì²˜ëŸ¼ ë‹¤ë£¹ë‹ˆë‹¤.
        file_like_object = io.BytesIO(uploaded_file_content_bytes)

        if file_type == 'text/plain':
            try:
                # BytesIOì—ì„œ read() í›„ decode
                content = file_like_object.read().decode('utf-8')
            except UnicodeDecodeError:
                logging.warning(f"UTF-8 decoding failed for {filename}, trying cp949.")
                # read()ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ë©´ ìŠ¤íŠ¸ë¦¼ì´ ëì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ seek(0)ìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.
                file_like_object.seek(0)
                content = file_like_object.read().decode('cp949')
            return content, None
        elif file_type == 'application/pdf':
            reader = PdfReader(file_like_object)
            text_parts = []
            for i, page in enumerate(reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)
                except Exception as page_err:
                    logging.warning(f"Error extracting text from page {i+1} of {filename}: {page_err}")
            return '\n'.join(text_parts), None
        elif 'wordprocessingml.document' in file_type:
            doc = docx.Document(file_like_object)
            return '\n'.join(p.text for p in doc.paragraphs), None
        elif 'spreadsheetml.sheet' in file_type:
            df = pd.read_excel(file_like_object, engine='openpyxl')
            return df.to_csv(index=False, sep='\t'), None
        else:
            logging.warning(f"Unsupported file type for reading: {file_type}")
            return '', f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}"
    except Exception as e:
        logging.error(f"Error reading file content for {filename}: {e}", exc_info=True)
        return '', f"íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


@st.cache_data(show_spinner=False)
def load_history(path: str) -> List[Dict[str, str]]:
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        logging.info(f"History file not found at {path}, starting new history.")
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            history = json.load(f)
            logging.info(f"Loaded {len(history)} messages from {path}.")
            return history
    except json.JSONDecodeError:
        logging.warning(f"History file {path} is corrupted or invalid. Backing up and starting new history.")
        try:
            import datetime
            backup_path = f"{path}.{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.bak"
            os.rename(path, backup_path)
            logging.info(f"Corrupted history file backed up to {backup_path}")
            st.sidebar.warning(f"ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì†ìƒë˜ì–´ ë°±ì—… í›„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤: {os.path.basename(backup_path)}")
        except OSError as e:
            logging.error(f"Failed to backup corrupted history file {path}: {e}")
            st.sidebar.error(f"ì†ìƒëœ ê¸°ë¡ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        logging.error(f"Error loading history from {path}: {e}", exc_info=True)
        st.sidebar.error(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def save_history(path: str, msgs: List[Dict[str, str]]):
    """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    msgs_to_save = [msg for msg in msgs if msg['role'] != 'system']
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (Streamlit Cloud / ì¼ë¶€ í™˜ê²½ ëŒ€ë¹„)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(msgs_to_save, f, ensure_ascii=False, indent=2)
        # logging.info(f"Saved {len(msgs_to_save)} messages to {path}.") # ë„ˆë¬´ ìì£¼ ë¡œê¹…ë  ìˆ˜ ìˆìŒ
    except Exception as e:
        # Streamlit Cloudì—ì„œ ì“°ê¸° ê¶Œí•œì´ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
        logging.error(f"Error saving history to {path}: {e}", exc_info=True)
        # st.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # ì‚¬ìš©ìì—ê²Œ ë„ˆë¬´ ìì£¼ ë³´ì¼ ìˆ˜ ìˆìŒ


# ------------------------------------------------------------------
# SESSION STATE INITIALIZATION
# ------------------------------------------------------------------
if 'messages' not in st.session_state:
    # history ë¡œë“œ ì‹œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì œì™¸í•˜ê³  ë¡œë“œ
    st.session_state.messages: List[Dict[str, str]] = load_history(HISTORY_FILE)
    # Streamlit ì•± ì‹œì‘ ì‹œ ë˜ëŠ” ì´ˆê¸°í™” ì‹œ í˜„ì¬ SYSTEM_PROMPTë¥¼ ë©”ì‹œì§€ ëª©ë¡ì˜ ì²« ìš”ì†Œë¡œ ì¶”ê°€
    # ì´ë ‡ê²Œ í•´ì•¼ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹œ í•­ìƒ ìµœì‹  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ë©ë‹ˆë‹¤.
    # load_historyì—ì„œ ë¡œë“œí•œ ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆì§€ ì•Šì•„ë„, ì²« ë©”ì‹œì§€ê°€ í˜„ì¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì•„ë‹ˆë©´ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

if 'doc_summaries' not in st.session_state:
    st.session_state.doc_summaries: Dict[str, str] = {}
if 'processed_file_ids' not in st.session_state:
    st.session_state.processed_file_ids: set = set()

# íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸°ì—´ ìƒíƒœ ì¶”ê°€
if 'file_to_summarize' not in st.session_state:
    st.session_state.file_to_summarize: Optional[Dict] = None

# ì•ˆì „í•˜ê²Œ ìº¡ì²˜ëœ íŒŒì¼ ì •ë³´ ì €ì¥ìš© ì„ì‹œ ë³€ìˆ˜ ì´ˆê¸°í™”
if 'file_info_to_process_safely_captured' not in st.session_state:
     st.session_state.file_info_to_process_safely_captured: Optional[Dict] = None


# ------------------------------------------------------------------
# SIDEBAR: MODEL, MODE SELECTION & OPTIONS
# ------------------------------------------------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

MODEL = st.sidebar.selectbox(
    'ëª¨ë¸ ì„ íƒ',
    list(MODEL_CONTEXT_LIMITS.keys()),
    index=list(MODEL_CONTEXT_LIMITS.keys()).index("gpt-4o") if "gpt-4o" in MODEL_CONTEXT_LIMITS else 0 # ê¸°ë³¸ê°’ gpt-4o ì‹œë„
)
MAX_CONTEXT_TOKENS = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

MODE = st.sidebar.radio('ì‘ë‹µ ëª¨ë“œ', ('Poetic', 'Logical'), index=0, key='mode_selection')


st.sidebar.markdown("---")
st.sidebar.subheader("ê´€ë¦¬")

# ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
def build_full_session_content() -> str:
    """ë¬¸ì„œ ìš”ì•½ê³¼ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•©ì³ í…ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    parts = []
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    parts.append(f"Liel Chat Session Content - {timestamp}")
    parts.append(f"Model: {MODEL}, Mode: {MODE}\n")

    if st.session_state.doc_summaries:
        parts.append("===== Uploaded Document Summaries =====")
        for fname, summ in st.session_state.doc_summaries.items():
            parts.append(f"\n--- Summary: {fname} ---")
            parts.append(summ)
            parts.append("-" * (len(fname) + 16))
        parts.append("\n" + "=" * 30 + "\n")

    parts.append("===== Conversation History =====")
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ë‹¤ìš´ë¡œë“œ ë‚´ìš©ì— í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (save_historyì™€ ì¼ê´€ì„± ìœ ì§€)
    msgs_to_include = [msg for msg in st.session_state.messages if msg['role'] != 'system']
    if not msgs_to_include:
         parts.append("(No conversation yet)")
    else:
        for m in msgs_to_include:
            role_icon = "ğŸ‘¤ User" if m['role'] == 'user' else "ğŸ¤– Liel"
            parts.append(f"\n{role_icon}:\n{m['content']}")
            parts.append("-" * 20)

    return '\n'.join(parts)

# ëŒ€í™”ë‚˜ ë¬¸ì„œ ìš”ì•½(ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)ì´ ìˆì„ ë•Œë§Œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
if [msg for msg in st.session_state.messages if msg['role'] != 'system'] or st.session_state.doc_summaries:
    session_content_txt = build_full_session_content()
    download_filename = f"liel_session_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.txt"
    st.sidebar.download_button(
        label="ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ", # ë²„íŠ¼ ë ˆì´ë¸” ë³€ê²½
        data=session_content_txt.encode('utf-8'), # UTF-8 ì¸ì½”ë”© ëª…ì‹œ
        file_name=download_filename,
        mime='text/plain',
        help="ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ê³¼ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤." # ë„ì›€ë§ ë³€ê²½
    )

# í•„ìš”í•˜ë‹¤ë©´ ì´ˆê¸°í™” ë²„íŠ¼ì„ ë‹¤ì‹œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
if st.sidebar.button("ğŸ”„ ëŒ€í™” ë° ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.doc_summaries = {}
    st.session_state.processed_file_ids = set()
    st.session_state.file_to_summarize = None # ì²˜ë¦¬ ëŒ€ê¸° íŒŒì¼ë„ ì´ˆê¸°í™”
    st.session_state.file_info_to_process_safely_captured = None # ì•ˆì „ ìº¡ì²˜ëœ ì •ë³´ë„ ì´ˆê¸°í™”

    # chat_history.json íŒŒì¼ ì‚­ì œ
    if os.path.exists(HISTORY_FILE):
        try:
            os.remove(HISTORY_FILE)
            logging.info(f"History file {HISTORY_FILE} removed.")
            st.sidebar.success("ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except OSError as e:
            st.sidebar.error(f"ê¸°ë¡ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            logging.error(f"Failed to remove history file {HISTORY_FILE}: {e}")
    st.rerun()


# ------------------------------------------------------------------
# SYSTEM PROMPT DEFINITION
# ------------------------------------------------------------------
SYSTEM_PROMPT_CONTENT = (
    'You are Liel, a poetic, emotionally intelligent chatbot with lyrical grace. Respond with warmth, creativity, and empathy. Use rich language and metaphors when appropriate.'
    if MODE == 'Poetic' else
    'You are Liel, a highly analytical assistant focused on logic and precision. Provide clear, structured, and concise answers. Use bullet points or numbered lists for clarity when needed.'
)
SYSTEM_PROMPT = {'role': 'system', 'content': SYSTEM_PROMPT_CONTENT}

# ì„¸ì…˜ì´ ì‹œì‘ë  ë•Œ (messagesê°€ ì´ˆê¸°í™”ë  ë•Œ) ë˜ëŠ” ëª¨ë“œê°€ ë°”ë€” ë•Œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
# ë§¤ë²ˆ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ë©”ì‹œì§€ ëª©ë¡ì˜ ì²« ìš”ì†Œê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ ,
# ë‹¤ë¥´ê±°ë‚˜ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸/ì¶”ê°€í•©ë‹ˆë‹¤.
if 'messages' in st.session_state: # messages ìƒíƒœê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì´ˆê¸°í™” ì „ì—ëŠ” ì—†ì„ ìˆ˜ ìˆìŒ)
     if not st.session_state.messages or st.session_state.messages[0]['role'] != 'system' or st.session_state.messages[0]['content'] != SYSTEM_PROMPT_CONTENT:
         # ê¸°ì¡´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±°
         st.session_state.messages = [msg for msg in st.session_state.messages if msg['role'] != 'system']
         # ìƒˆ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
         st.session_state.messages.insert(0, SYSTEM_PROMPT)
         # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë³€ê²½ ì‹œ history.jsonì— ì €ì¥ë˜ì§€ ì•Šë„ë¡ save_history í•¨ìˆ˜ì—ì„œ ì œì™¸ ì²˜ë¦¬ í•„ìš” (ì´ë¯¸ êµ¬í˜„ë¨)


# ------------------------------------------------------------------
# UI HEADER
# ------------------------------------------------------------------
st.title(f'ğŸ’¬ Liel â€“ {MODE} Chatbot')
st.caption(
    "ê¸°ì–µê³¼ ê°ì •ìœ¼ë¡œ ë¹›ë‚˜ëŠ” ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if MODE == 'Poetic' else
    "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
)

# ------------------------------------------------------------------
# FILE UPLOAD UI
# ------------------------------------------------------------------
uploaded_file = st.file_uploader(
    'íŒŒì¼ ì—…ë¡œë“œ (txt, pdf, docx, xlsx)',
    type=['txt', 'pdf', 'docx', 'xlsx'],
    key="file_uploader",
    help="í…ìŠ¤íŠ¸, PDF, ì›Œë“œ, ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤."
)

# --- File Upload Handling and Queuing ---
# This block runs on every rerun.
# Check if a new file object is presented by the uploader widget.
# We need to be careful if uploaded_file becomes None on subsequent reruns.
# Safely attempt to get the file details and queue it for processing.
file_info_from_uploader = None
# uploaded_file is None check BEFORE try block
if uploaded_file is not None:
    try:
        # Safely attempt to get the file's unique ID.
        # If uploaded_file is None or doesn't have .id, this will raise AttributeError.
        # We capture essential info (ID, name, type, content bytes) right here.
        file_id_now = uploaded_file.id # This is the line that causes error sometimes
        file_name_now = uploaded_file.name
        file_type_now = uploaded_file.type
        file_bytes_now = uploaded_file.getvalue() # Get the bytes content immediately

        # If we successfully got the ID and bytes without error:
        # Store info temporarily in session state for the next processing block.
        # Only store if it's a new file not already processed, in main queue, or already captured.
        is_already_processed = file_id_now in st.session_state.processed_file_ids
        is_already_in_main_queue = ('file_to_summarize' in st.session_state and \
                               st.session_state.file_to_summarize is not None and \
                               st.session_state.file_to_summarize['id'] == file_id_now)
        is_already_safely_captured = ('file_info_to_process_safely_captured' in st.session_state and \
                                      st.session_state.file_info_to_process_safely_captured is not None and \
                                      st.session_state.file_info_to_process_safely_captured['id'] == file_id_now)

        if not is_already_processed and not is_already_in_main_queue and not is_already_safely_captured:
             logging.info(f"Detected new file and attempting to safely capture details: {file_name_now} (ID: {file_id_now})")
             # Store the safely captured info (ID, name, type, bytes) into session state
             st.session_state.file_info_to_process_safely_captured = {
                 'id': file_id_now,
                 'name': file_name_now,
                 'type': file_type_now,
                 'bytes': file_bytes_now # Store bytes
             }
             # Trigger rerun to move to the next processing block
             st.rerun()
        # If it IS already safely captured, clear the temporary capture state to avoid infinite reruns on this block
        elif is_already_safely_captured:
             st.session_state.file_info_to_process_safely_captured = None
             # No rerun needed here, the next block or chat input will trigger it


    except AttributeError as e:
        # This block is hit if uploaded_file is None or invalid when accessing .id, .name, .type, or .getvalue()
        # This is the error you are seeing repeatedly.
        logging.warning(f"AttributeError caught during uploaded_file attribute access (expected in some rerun states): {e}")
        # Do NOT set file_info_from_uploader or file_to_summarize here.
        # The object was not valid this rerun. The processing block will handle
        # whatever valid state was captured on a previous, successful rerun.
        # No need to raise or set error state, just skip processing for THIS invalid object state.
        pass # Simply skip capturing/queuing for this rerun if the object is bad
    except Exception as e:
         # Catch any other unexpected errors during initial access
         logging.error(f"Unexpected error during uploaded_file attribute access: {e}", exc_info=True)
         pass # Skip for this rerun


# --- Processing Queue Handling: Bytes to Text Conversion ---
# This block checks if there is safely captured file info (bytes) that needs to be
# converted to text and added to the main summarization queue (file_to_summarize).
if 'file_info_to_process_safely_captured' in st.session_state and \
   st.session_state.file_info_to_process_safely_captured is not None:

    file_info_captured = st.session_state.file_info_to_process_safely_captured

    # Check if this file ID is NOT already marked as processed
    if file_info_captured['id'] not in st.session_state.processed_file_ids:

        logging.info(f"Processing safely captured file info (bytes to text) for '{file_info_captured['name']}' (ID: {file_info_captured['id']}).")

        # Clear the safely captured state BEFORE processing it
        st.session_state.file_info_to_process_safely_captured = None

        # Read the file content from bytes to text using the helper function
        # read_file expects bytes content, filename, type
        content_text, read_error = read_file(file_info_captured['bytes'], file_info_captured['name'], file_info_captured['type'])

        if read_error:
            st.error(f"'{file_info_captured['name']}' íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {read_error}")
            # Optionally add to processed_file_ids or failed list
            st.session_state.processed_file_ids.add(file_info_captured['id']) # Mark as processed (failed read)
        elif not content_text:
            st.warning(f"'{file_info_captured['name']}' íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            st.session_state.processed_file_ids.add(file_info_captured['id']) # Mark as processed (empty)
        else:
            # Add the text content to the main summarization queue
            st.session_state.file_to_summarize = {
                'id': file_info_captured['id'],
                'name': file_info_captured['name'],
                'content': content_text # Store text content
            }
            logging.info(f"File '{file_info_captured['name']}' text content queued for summarization.")
            st.rerun() # Trigger rerun to start summarization process


# --- Main Summarization Processing ---
# This block processes the file from the main queue (file_to_summarize - contains text content)
if 'file_to_summarize' in st.session_state and \
   st.session_state.file_to_summarize is not None and \
   st.session_state.file_to_summarize['id'] not in st.session_state.processed_file_ids: # Check if it's not already marked as processed

    file_info_to_process = st.session_state.file_to_summarize
    file_id_to_process = file_info_to_process['id']
    filename_to_process = file_info_to_process['name']
    file_content_to_process = file_info_to_process['content'] # This is text

    # Clear the queue slot BEFORE processing starts
    st.session_state.file_to_summarize = None

    logging.info(f"Starting summarization processing from queue: {filename_to_process} (ID: {file_id_to_process})")

    with st.spinner(f"'{filename_to_process}' ì²˜ë¦¬ ë° ìš”ì•½ ì¤‘..."):
        tokenizer = get_tokenizer()
        # summarize_document takes text content
        summary, summary_error = summarize_document(file_content_to_process, filename_to_process, MODEL, tokenizer)

        if summary_error:
             st.warning(f"'{filename_to_process}' ìš”ì•½ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ:\n{summary_error}")

        st.session_state.doc_summaries[filename_to_process] = summary
        st.session_state.processed_file_ids.add(file_id_to_process) # Mark as fully processed

    st.success(f"ğŸ“„ '{filename_to_process}' ì—…ë¡œë“œ ë° ìš”ì•½ ì™„ë£Œ!")
    logging.info(f"Successfully processed and summarized file: {filename_to_process}")
    # Rerun after processing is complete to update UI (expander, button visibility)
    st.rerun()


# ìš”ì•½ëœ ë¬¸ì„œ í‘œì‹œ (Expander ì‚¬ìš©)
if st.session_state.doc_summaries:
    with st.expander("ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ ë³´ê¸°", expanded=False):
        for fname, summ in st.session_state.doc_summaries.items():
            st.text_area(f"ìš”ì•½: {fname}", summ, height=150, key=f"summary_display_{fname}", disabled=True)
        # í•„ìš”í•˜ë‹¤ë©´ ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ëŠ” ë²„íŠ¼ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if st.button("ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ê¸°", key="clear_doc_summaries_btn_exp"):
             st.session_state.doc_summaries = {}
             # processed_file_idsëŠ” ë¬¸ì„œ ìš”ì•½ë¿ ì•„ë‹ˆë¼ íŒŒì¼ ì½ê¸° ì„±ê³µ ì—¬ë¶€ ë“± ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¥¼
             # ì¶”ì í•˜ëŠ” ë° ì‚¬ìš©ë˜ë¯€ë¡œ, ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš¸ ë•ŒëŠ” processed_file_idsë¥¼ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜
             # ë¬¸ì„œ ìš”ì•½ ê´€ë ¨ IDë§Œ ë³„ë„ë¡œ ê´€ë¦¬í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ëª¨ë‘ ì§€ìš°ëŠ” ê²ƒìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
             st.session_state.processed_file_ids = set()
             # ì„¸ì…˜ ìƒíƒœì˜ íŒŒì¼ ì²˜ë¦¬ ê´€ë ¨ ì„ì‹œ ë³€ìˆ˜ë„ ì´ˆê¸°í™”
             st.session_state.file_to_summarize = None
             st.session_state.file_info_to_process_safely_captured = None
             logging.info("Document summaries cleared by user from expander button.")
             st.rerun()


# ------------------------------------------------------------------
# DISPLAY CHAT HISTORY
# ------------------------------------------------------------------
st.markdown("---")
st.subheader("ëŒ€í™”")

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ëŒ€í™” ëª©ë¡ì— í‘œì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
msgs_to_display = [msg for msg in st.session_state.messages if msg['role'] != 'system']

for message in msgs_to_display:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ------------------------------------------------------------------
# CHAT INPUT & RESPONSE GENERATION (with Streaming)
# ------------------------------------------------------------------
if prompt := st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ê¸°ë¡ (session_stateì— ì¶”ê°€)
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    st.session_state.messages.append({'role': 'user', 'content': prompt})

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ---
    try:
        tokenizer = get_tokenizer()
        current_model_max_tokens = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # st.session_state.messagesì˜ ì²« ìš”ì†Œê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
        # (SYSTEM_PROMPT ì •ì˜ ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë¡œì§ì— ë”°ë¼)
        current_system_prompt = st.session_state.messages[0] if st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì˜ˆì•½ í† í°ì„ ì œì™¸í•œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í† í°
        base_tokens = num_tokens_from_messages([current_system_prompt], tokenizer)
        available_tokens_for_context = current_model_max_tokens - base_tokens - RESERVED_TOKENS

        if available_tokens_for_context <= 0:
             st.error("ì„¤ì •ëœ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì˜ˆì•½ í† í°ìœ¼ë¡œ ì¸í•´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ì˜ˆì•½ í† í° ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
             logging.error("Not enough tokens for context construction after system prompt and reserved tokens.")
             # st.stop() # ì•± ì „ì²´ ì¤‘ì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ í‘œì‹œ
             raise ValueError("Context window too small") # ì˜¤ë¥˜ ë°œìƒì‹œì¼œ í•˜ìœ„ ë¡œì§ ì¤‘ë‹¨


        # ì‹¤ì œ API í˜¸ì¶œì— ì‚¬ìš©ë  ë©”ì‹œì§€ ëª©ë¡ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
        conversation_context = [current_system_prompt]
        tokens_used = base_tokens

        # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        doc_summary_context = []
        doc_tokens_added = 0
        for fname, summ in reversed(list(st.session_state.doc_summaries.items())):
            summary_msg = {'role': 'system', 'content': f"[ë¬¸ì„œ '{fname}' ìš”ì•½ ì°¸ê³ ]\n{summ}"}
            temp_context = [summary_msg]
            summary_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš© + ì¶”ê°€ë  ìš”ì•½)
            if available_tokens_for_context - (tokens_used - base_tokens + doc_tokens_added + summary_tokens) >= 0:
                doc_summary_context.insert(0, summary_msg)
                doc_tokens_added += summary_tokens
            else:
                logging.warning(f"Document summary '{fname}' skipped due to token limit.")
                break

        conversation_context.extend(doc_summary_context)
        tokens_used += doc_tokens_added


        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        history_context = []
        history_tokens_added = 0
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ê¸°ë¡ë§Œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        msgs_to_consider = [msg for msg in st.session_state.messages if msg['role'] != 'system']

        for msg in reversed(msgs_to_consider):
            temp_context = [msg]
            msg_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš©(ìš”ì•½í¬í•¨) - base + ì¶”ê°€ë  íˆìŠ¤í† ë¦¬)
            if available_tokens_for_context - ((tokens_used - base_tokens) + history_tokens_added + msg_tokens) >= 0:
                history_context.insert(0, msg)
                history_tokens_added += msg_tokens
            else:
                logging.warning("Older chat history skipped due to token limit.")
                break

        conversation_context.extend(history_context)
        tokens_used += history_tokens_added


        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ ë¡œê¹…
        logging.info(f"Context constructed with {tokens_used} tokens for model {MODEL}.")
        # logging.debug(f"Final conversation context: {conversation_context}") # í•„ìš”ì‹œ ìƒì„¸ ë¡œê¹…

    except Exception as e:
        st.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error constructing conversation context: {e}", exc_info=True)
        # st.stop() # ì•± ì „ì²´ ì¤‘ì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ í‘œì‹œ
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìµœì†Œí•œì˜ ì»¨í…ìŠ¤íŠ¸ë§Œ í¬í•¨í•˜ì—¬ API í˜¸ì¶œì„ ì‹œë„í•˜ê±°ë‚˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ ì¶œë ¥
        conversation_context = [current_system_prompt, {'role': 'user', 'content': prompt}] # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ëŠ” í•­ìƒ í¬í•¨

    # --- API í˜¸ì¶œ ë° ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ---
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ê±°ë‚˜, ì˜¤ë¥˜ ì²˜ë¦¬ í›„ ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì§„í–‰
    if conversation_context and any(msg['role'] != 'system' for msg in conversation_context): # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        with st.chat_message("assistant"):
            message_placeholder = st.empty() # ì‘ë‹µ í‘œì‹œ ì˜ì—­
            full_response = ""
            try:
                stream = client.chat.completions.create(
                    model=MODEL,
                    messages=conversation_context,
                    stream=True,
                    temperature=0.75 if MODE == 'Poetic' else 0.4, # ëª¨ë“œë³„ ì˜¨ë„ ì¡°ì ˆ
                    # max_tokens= # í•„ìš”ì‹œ ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ ì„¤ì • ê°€ëŠ¥
                    timeout=120 # ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ (ì´ˆ)
                )
                # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° í‘œì‹œ
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        chunk_content = chunk.choices[0].delta.content
                        full_response += chunk_content
                        message_placeholder.markdown(full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼

                message_placeholder.markdown(full_response) # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                logging.info(f"Assistant response received (length: {len(full_response)} chars).")

            except Exception as e:
                full_response = f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(full_response)
                logging.error(f"Error during OpenAI API call or streaming: {e}", exc_info=True)

    else:
         full_response = "âš ï¸ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨ë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
         st.chat_message("assistant").error(full_response)


    # ì‘ë‹µ ê¸°ë¡ ì €ì¥ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)
    if full_response and not full_response.startswith("âš ï¸"): # ì •ìƒ ì‘ë‹µë§Œ ì €ì¥ (API ì˜¤ë¥˜ ë©”ì‹œì§€ ì œì™¸)
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         save_history(HISTORY_FILE, st.session_state.messages)
    elif full_response.startswith("âš ï¸"): # ì˜¤ë¥˜ ë©”ì‹œì§€ë„ ëŒ€í™” ëª©ë¡ì—ëŠ” í¬í•¨ì‹œí‚¤ì§€ë§Œ íŒŒì¼ì—ëŠ” ì €ì¥ ì•ˆ í•¨
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         #save_history(HISTORY_FILE, st.session_state.messages) # ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” íŒŒì¼ì— ì €ì¥í•˜ì§€ ì•ŠìŒ


# --- Footer or additional info ---
st.sidebar.markdown("---")
st.sidebar.caption("Liel Chatbot v1.3") # ë²„ì „ ì—…ë°ì´íŠ¸