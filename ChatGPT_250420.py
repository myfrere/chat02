import json
import os
import streamlit as st
from openai import OpenAI
from pypdf import PdfReader
import docx
import pandas as pd
from time import sleep
import tiktoken
import logging
from typing import List, Dict, Tuple, Optional, Any
import io
import base64

# ------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------------------------------------------
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# ------------------------------------------------------------------
# CONFIGURATION & CONSTANTS
# ------------------------------------------------------------------
MODEL_CONTEXT_LIMITS = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4o": 128000,
    "gpt-4-turbo": 128000,
}
MULTIMODAL_VISION_MODELS = ["gpt-4o", "gpt-4-turbo"] # ë©€í‹°ëª¨ë‹¬ ì§€ì› ëª¨ë¸ ëª©ë¡

DEFAULT_ENCODING = "cl100k_base"
CHUNK_SIZE = 2000
RESERVED_TOKENS = 1500
HISTORY_FILE = "history/chat_history.json"
CHUNK_PROMPT_FOR_SUMMARY = 'Summarize the key points of this text chunk in 2-3 concise bullet points, focusing on the main information.'

# ------------------------------------------------------------------
# STREAMLIT PAGE SETUP
# ------------------------------------------------------------------
st.set_page_config(page_title="Liel â€“ AI Chatbot", layout="wide", initial_sidebar_state="auto")

# ------------------------------------------------------------------
# OPENAI CLIENT INITIALIZATION & API KEY HANDLING
# ------------------------------------------------------------------
def initialize_openai_client() -> Optional[OpenAI]:
    """Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = None
    try:
        # 1. Streamlit Secrets í™•ì¸ (í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ê¶Œì¥)
        if "general" in st.secrets and "OPENAI_API_KEY" in st.secrets["general"]:
            api_key = st.secrets["general"]["OPENAI_API_KEY"]
            logging.info("API Key loaded from Streamlit Secrets.")
        # 2. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Secretsì— ì—†ê±°ë‚˜ ë¡œì»¬ ì‹¤í–‰ ì‹œ)
        elif os.environ.get("OPENAI_API_KEY"):
            api_key = os.environ.get("OPENAI_API_KEY")
            logging.info("API Key loaded from environment variable.")

    except Exception as e:
        logging.warning(f"Error accessing API key from secrets or env: {e}")

    if not api_key or not api_key.startswith("sk-"):
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        st.warning("API í‚¤ë¥¼ `.streamlit/secrets.toml` íŒŒì¼ì— `[general]\nOPENAI_API_KEY='sk-...'` í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return None

    try:
        client = OpenAI(api_key=api_key)
        logging.info("OpenAI client initialized.")
        return client
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error(f"Client initialization failed: {e}", exc_info=True)
        return None

client = initialize_openai_client()

if client is None:
    st.stop() # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# ------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------
def get_tokenizer(encoding_name: str = DEFAULT_ENCODING) -> tiktoken.Encoding:
    """ì§€ì •ëœ ì´ë¦„ì˜ tiktoken ì¸ì½”ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return tiktoken.get_encoding(encoding_name)
    except ValueError:
        logging.warning(f"Encoding '{encoding_name}' not found. Using default '{DEFAULT_ENCODING}'.")
        return tiktoken.get_encoding(DEFAULT_ENCODING)

def num_tokens_from_string(string: str, encoding: tiktoken.Encoding) -> int:
    """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return len(encoding.encode(string)) if string else 0

# num_tokens_from_messages remains the same (estimates text tokens only)
def num_tokens_from_messages(messages: List[Dict[str, Any]], encoding: tiktoken.Encoding) -> int:
    """Calculates tokens for text parts in messages (simplified for multimodal)."""
    num_tokens = 0
    for message in messages:
        num_tokens += 4 # base tokens per message

        content = message.get("content")
        if isinstance(content, str):
             num_tokens += num_tokens_from_string(content, encoding)
        elif isinstance(content, list):
             for part in content:
                 if part.get("type") == "text" and "text" in part:
                      num_tokens += num_tokens_from_string(part["text"], encoding)
                 # Image tokens are NOT calculated here.

        if "name" in message:
            num_tokens -= 1

    num_tokens += 2 # assistant reply start tokens
    return num_tokens


# read_file remains the same
@st.cache_data(show_spinner=False, hash_funcs={docx.document.Document: id, pd.DataFrame: pd.util.hash_pandas_object})
def read_file(uploaded_file_content_bytes, filename, file_type) -> Tuple[str, Optional[str]]:
    """ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë°”ì´íŠ¸ ë‚´ìš©ì„ ì½ì–´ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)."""
    try:
        logging.info(f"Reading file: {filename} (Type: {file_type})")
        file_like_object = io.BytesIO(uploaded_file_content_bytes)

        if file_type == 'text/plain':
            try:
                return file_like_object.read().decode('utf-8'), None
            except UnicodeDecodeError:
                file_like_object.seek(0)
                return file_like_object.read().decode('cp949'), None
        elif file_type == 'application/pdf':
            reader = PdfReader(file_like_object)
            text_parts = [page.extract_text() or '' for page in reader.pages]
            return '\n'.join(part for part in text_parts if part), None
        elif 'wordprocessingml.document' in file_type:
            doc = docx.Document(file_like_object)
            return '\n'.join(p.text for p in doc.paragraphs), None
        elif 'spreadsheetml.sheet' in file_type:
            df = pd.read_excel(file_like_object, engine='openpyxl')
            return df.to_csv(index=False, sep='\t'), None
        elif file_type in ['image/jpeg', 'image/png']:
             return '', f"ì´ë¯¸ì§€ íŒŒì¼ì€ í…ìŠ¤íŠ¸ ë³€í™˜ ì§€ì› ì•ˆ í•¨: {file_type}"
        else:
            return '', f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}"
    except Exception as e:
        logging.error(f"Error reading file {filename}: {e}", exc_info=True)
        return '', f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# summarize_document remains the same
@st.cache_data(show_spinner=False)
def summarize_document(text: str, filename: str, model: str, tokenizer: tiktoken.Encoding) -> Tuple[str, Optional[str]]:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ë¡œ ìš”ì•½."""
    if not text:
        return "(ë¬¸ì„œ ë‚´ìš© ì—†ìŒ)", None

    summaries = []
    chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]
    total_chunks = len(chunks)
    logging.info(f"Starting summarization for '{filename}' ({total_chunks} chunks).")

    progress_bar = st.progress(0, text=f"'{filename}' ìš”ì•½ ì¤‘... [0/{total_chunks}]")
    summary_errors = []

    for i, chunk in enumerate(chunks):
        progress = (i + 1) / total_chunks
        progress_bar.progress(progress, text=f"'{filename}' ìš”ì•½ ì¤‘... [{i+1}/{total_chunks}]")

        chunk_tokens = num_tokens_from_string(chunk, tokenizer)
        model_limit = MODEL_CONTEXT_LIMITS.get(model, 8192)
        if chunk_tokens > model_limit - 500:
             logging.warning(f"Chunk {i+1} tokens ({chunk_tokens}) near/exceeds model limit ({model}).")

        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {'role': 'system', 'content': CHUNK_PROMPT_FOR_SUMMARY},
                    {'role': 'user', 'content': chunk}
                ],
                max_tokens=250,
                temperature=0.3,
                timeout=60
            )
            if response.choices and response.choices[0].message and response.choices[0].message.content:
                summaries.append(response.choices[0].message.content.strip())
            else:
                 logging.warning(f"Chunk {i+1} summarization returned no content.")
                 summaries.append(f"(ì²­í¬ {i+1} ìš”ì•½ ë‚´ìš© ì—†ìŒ)")

            sleep(0.15)
        except Exception as e:
            error_msg = f"ì²­í¬ {i+1} ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.warning(error_msg)
            logging.error(f"Error summarizing chunk {i+1}: {e}", exc_info=True)
            summaries.append(f"(ì²­í¬ {i+1} ìš”ì•½ ì‹¤íŒ¨)")
            summary_errors.append(error_msg)

    progress_bar.empty()
    return '\n'.join(summaries), "\n".join(summary_errors) if summary_errors else None

# load_history remains the same
@st.cache_data(show_spinner=False)
def load_history(path: str) -> List[Dict[str, str]]:
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        logging.info(f"History file not found at {path}.")
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            history = json.load(f)
            logging.info(f"Loaded {len(history)} messages from {path}.")
            # Ensure loaded messages have 'content' as list if needed for multimodal history display
            # (This is a simplification; a real app might need migration logic)
            # For now, assume history file only contains text strings for content.
            # The display logic below handles list content correctly.
            return history
    except (json.JSONDecodeError, FileNotFoundError, OSError) as e:
        logging.warning(f"Error loading history file {path}: {e}")
        if os.path.exists(path):
             st.sidebar.warning(f"ëŒ€í™” ê¸°ë¡ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ìƒˆ ê¸°ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ({os.path.basename(path)})")
        return []
    except Exception as e:
        logging.error(f"Unexpected error loading history from {path}: {e}", exc_info=True)
        st.sidebar.error(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


# save_history remains the same (only saves text messages)
def save_history(path: str, msgs: List[Dict[str, Any]]):
    """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë° ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ì œì™¸)."""
    # í˜„ì¬ save_historyëŠ” contentê°€ ë¬¸ìì—´ì¸ ë©”ì‹œì§€ë§Œ ì €ì¥í•˜ë„ë¡ í•„í„°ë§í•©ë‹ˆë‹¤.
    # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ë ¤ë©´ ì €ì¥ í˜•ì‹ì„ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ save_history ë¡œì§ ìœ ì§€ (í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë§Œ ì €ì¥)
    msgs_to_save = [msg for msg in msgs if msg['role'] != 'system' and isinstance(msg.get('content'), str)]

    if not msgs_to_save:
        if os.path.exists(path):
            try:
                os.remove(path)
                logging.info(f"History file {path} removed as messages are empty.")
            except OSError as e:
                logging.error(f"Failed to remove empty history file {path}: {e}")
        return

    history_dir = os.path.dirname(path)
    if history_dir:
        try:
            os.makedirs(history_dir, exist_ok=True)
        except Exception as e:
             logging.error(f"Error creating history directory {history_dir}: {e}", exc_info=True)
             st.sidebar.error(f"ê¸°ë¡ í´ë” ìƒì„± ì‹¤íŒ¨: {e}. ê¸°ë¡ ì €ì¥ì´ ì•ˆ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
             return

    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(msgs_to_save, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.error(f"Error saving history to {path}: {e}", exc_info=True)


# ------------------------------------------------------------------
# SESSION STATE INITIALIZATION
# ------------------------------------------------------------------
if 'messages' not in st.session_state:
    st.session_state.messages: List[Dict[str, Any]] = load_history(HISTORY_FILE)
    logging.info(f"Session initialized. Loaded {len(st.session_state.messages)} messages.")

if 'doc_summaries' not in st.session_state:
    st.session_state.doc_summaries: Dict[str, str] = {}
if 'processed_file_ids' not in st.session_state:
    st.session_state.processed_file_ids: set = set()

if 'file_to_summarize' not in st.session_state:
    st.session_state.file_to_summarize: Optional[Dict] = None

if 'file_info_to_process_safely_captured' not in st.session_state:
     st.session_state.file_info_to_process_safely_captured: Optional[Dict] = None

# State for image data waiting for the next prompt
if 'uploaded_image_for_next_prompt' not in st.session_state:
    st.session_state.uploaded_image_for_next_prompt: Optional[Dict] = None # Stores {'id':..., 'name':..., 'type': ..., 'bytes': ...}


# ------------------------------------------------------------------
# SIDEBAR: MODEL, MODE SELECTION & OPTIONS
# ------------------------------------------------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

# Filter model selection to only include multimodal models
MODEL = st.sidebar.selectbox(
    'ëª¨ë¸ ì„ íƒ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)',
    MULTIMODAL_VISION_MODELS, # Use the filtered list
    index=MULTIMODAL_VISION_MODELS.index("gpt-4o") if "gpt-4o" in MULTIMODAL_VISION_MODELS else 0 # Default to gpt-4o
)
MAX_CONTEXT_TOKENS = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

MODE = st.sidebar.radio('ì‘ë‹µ ëª¨ë“œ', ('Poetic', 'Logical'), index=0, key='mode_selection')

st.sidebar.markdown("---")
st.sidebar.subheader("ê´€ë¦¬")

# build_full_session_content remains the same (formats messages back to text)
def build_full_session_content() -> str:
    """ë¬¸ì„œ ìš”ì•½ê³¼ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•©ì³ í…ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    parts = []
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    parts.append(f"Liel Chat Session Content - {timestamp}")
    parts.append(f"Model: {MODEL}, Mode: {MODE}\n")

    if st.session_state.doc_summaries:
        parts.append("===== Uploaded Document Summaries =====")
        for fname in sorted(st.session_state.doc_summaries.keys()):
            summ = st.session_state.doc_summaries[fname]
            parts.append(f"\n--- Summary: {fname} ---")
            parts.append(summ)
            parts.append("-" * (len(fname) + 16))
        parts.append("\n" + "=" * 30 + "\n")

    parts.append("===== Conversation History =====")
    msgs_to_include = [msg for msg in st.session_state.messages if msg['role'] != 'system']
    if not msgs_to_include:
         parts.append("(No conversation yet)")
    else:
        for m in msgs_to_include:
            role_icon = "ğŸ‘¤ User" if m['role'] == 'user' else "ğŸ¤– Liel"
            parts.append(f"\n{role_icon}:\n")
            # Handle different content types for download text
            content = m.get('content')
            if isinstance(content, str):
                 parts.append(content)
            elif isinstance(content, list):
                 # For download, represent multimodal parts as text descriptions
                 multimodal_parts = []
                 for part in content:
                     if part.get("type") == "text" and "text" in part:
                         multimodal_parts.append(part["text"])
                     elif part.get("type") == "image_url" and "image_url" in part:
                         multimodal_parts.append("[Uploaded Image]") # Or some representation
                 parts.append("\n".join(multimodal_parts))
            else:
                 parts.append("(Unsupported message content format)")

            parts.append("-" * 20)

    return '\n'.join(parts)


# Download Button remains the same (uses build_full_session_content)
if [msg for msg in st.session_state.messages if msg['role'] != 'system'] or st.session_state.doc_summaries:
    session_content_txt = build_full_session_content()
    download_filename = f"liel_session_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.txt"
    st.sidebar.download_button(
        label="ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ",
        data=session_content_txt.encode('utf-8'),
        file_name=download_filename,
        mime='text/plain',
        help="ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ê³¼ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
    )

# Clear Button remains the same
if st.sidebar.button("ğŸ”„ ëŒ€í™” ë° ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.doc_summaries = {}
    st.session_state.processed_file_ids = set()
    st.session_state.file_to_summarize = None
    st.session_state.file_info_to_process_safely_captured = None
    st.session_state.uploaded_image_for_next_prompt = None # Clear pending image

    if os.path.exists(HISTORY_FILE):
        try:
            os.remove(HISTORY_FILE)
            logging.info(f"History file {HISTORY_FILE} removed.")
            st.sidebar.success("ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except OSError as e:
            logging.error(f"Failed to remove history file {HISTORY_FILE}: {e}")
            st.sidebar.error(f"ê¸°ë¡ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    st.rerun()


# ------------------------------------------------------------------
# SYSTEM PROMPT DEFINITION
# ------------------------------------------------------------------
SYSTEM_PROMPT_CONTENT = (
    'You are Liel, a poetic, emotionally intelligent chatbot with lyrical grace. Respond with warmth, creativity, and empathy. Use rich language and metaphors when appropriate.'
    if MODE == 'Poetic' else
    'You are Liel, a highly analytical assistant focused on logic and precision. Provide clear, structured, and concise answers. Use bullet points or numbered lists for clarity when needed.'
)
SYSTEM_PROMPT = {'role': 'system', 'content': SYSTEM_PROMPT_CONTENT}

# System prompt management in messages state remains the same
if 'messages' in st.session_state:
     st.session_state.messages = [msg for msg in st.session_state.messages if msg['role'] != 'system']
     st.session_state.messages.insert(0, SYSTEM_PROMPT)


# ------------------------------------------------------------------
# UI HEADER
# ------------------------------------------------------------------
st.title(f'ğŸ’¬ Liel â€“ {MODE} Chatbot')
st.caption(
    "ê¸°ì–µê³¼ ê°ì •ìœ¼ë¡œ ë¹›ë‚˜ëŠ” ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if MODE == 'Poetic' else
    "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
)

# ------------------------------------------------------------------
# FILE UPLOAD UI
# ------------------------------------------------------------------
uploaded_file = st.file_uploader(
    'íŒŒì¼ ì—…ë¡œë“œ (txt, pdf, docx, xlsx, jpg, png)',
    type=['txt', 'pdf', 'docx', 'xlsx', 'jpg', 'png'],
    key="file_uploader",
    help="í…ìŠ¤íŠ¸, PDF, ì›Œë“œ, ì—‘ì…€ íŒŒì¼ì€ ìš”ì•½í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ê³ , ì´ë¯¸ì§€ íŒŒì¼ì€ ë‹¤ìŒ ì§ˆë¬¸ê³¼ í•¨ê»˜ ëª¨ë¸ì—ê²Œ ì „ì†¡í•©ë‹ˆë‹¤."
)

# --- File Upload Handling and Queuing: Step 1 - Safely Capture File Info ---
if uploaded_file is not None:
    try:
        file_id_now = uploaded_file.id
        file_name_now = uploaded_file.name
        file_type_now = uploaded_file.type
        file_bytes_now = uploaded_file.getvalue()

        is_already_processed = file_id_now in st.session_state.processed_file_ids
        is_already_in_main_queue = st.session_state.get('file_to_summarize', None) is not None and st.session_state.file_to_summarize['id'] == file_id_now
        is_already_safely_captured = st.session_state.get('file_info_to_process_safely_captured', None) is not None and st.session_state.file_info_to_process_safely_captured['id'] == file_id_now
        is_current_image_for_prompt = st.session_state.get('uploaded_image_for_next_prompt', None) is not None and st.session_state.uploaded_image_for_next_prompt.get('id') == file_id_now

        if not is_already_processed and not is_already_in_main_queue and not is_already_safely_captured and not is_current_image_for_prompt:
            logging.info(f"Detected new file, capturing details: {file_name_now} (ID: {file_id_now})")
            st.session_state.file_info_to_process_safely_captured = {
                'id': file_id_now, 'name': file_name_now, 'type': file_type_now, 'bytes': file_bytes_now
            }
            st.rerun()
        elif is_already_safely_captured and not is_already_processed and not is_already_in_main_queue and not is_current_image_for_prompt:
             logging.info(f"File '{file_name_now}' (ID: {file_id_now}) already safely captured. Clearing capture state.")
             st.session_state.file_info_to_process_safely_captured = None
             pass # Next processing step will handle it

    except AttributeError as e:
        logging.warning(f"AttributeError caught during uploaded_file access: {e}")
        pass
    except Exception as e:
         logging.error(f"Unexpected error during uploaded_file access: {e}", exc_info=True)
         pass


# --- File Upload Handling and Queuing: Step 2 - Process Captured Info (Bytes to Text or Queue Image) ---
if st.session_state.get('file_info_to_process_safely_captured', None) is not None:
    file_info_captured = st.session_state.file_info_to_process_safely_captured

    if file_info_captured['id'] not in st.session_state.processed_file_ids:

        logging.info(f"Processing safely captured file info: {file_info_captured['name']} (Bytes to Text/Queue Image).")
        st.session_state.file_info_to_process_safely_captured = None

        # Handle image files: store bytes to be sent with the next prompt
        if file_info_captured['type'] in ['image/jpeg', 'image/png']:
             logging.info(f"Queuing image file for next prompt: {file_info_captured['name']}")

             st.session_state.uploaded_image_for_next_prompt = {
                 'id': file_info_captured['id'],
                 'name': file_info_captured['name'],
                 'type': file_info_captured['type'],
                 'bytes': file_info_captured['bytes']
             }

             st.info(f"âœ¨ ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ ì§ˆë¬¸ê³¼ í•¨ê»˜ ëª¨ë¸ì—ê²Œ ì „ì†¡ë©ë‹ˆë‹¤.")
             st.session_state.processed_file_ids.add(file_info_captured['id'])

             # Display the image in the chat history area immediately
             with st.chat_message("user"):
                 st.image(file_info_captured['bytes'], caption=f"ì—…ë¡œë“œëœ ì´ë¯¸ì§€: {file_info_captured['name']}", use_column_width=True)

             st.rerun() # Update UI and wait for user prompt

        else: # Handle text-based files: Read content and queue for summarization
            content_text, read_error = read_file(file_info_captured['bytes'], file_info_captured['name'], file_info_captured['type'])

            if read_error:
                st.error(f"'{file_info_captured['name']}' íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {read_error}")
                st.session_state.processed_file_ids.add(file_info_captured['id'])
            elif not content_text:
                st.warning(f"'{file_info_captured['name']}' íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                st.session_state.processed_file_ids.add(file_info_captured['id'])
            else:
                st.session_state.file_to_summarize = {
                    'id': file_info_captured['id'], 'name': file_info_captured['name'], 'content': content_text
                }
                logging.info(f"File '{file_info_captured['name']}' text content queued for summarization.")
                st.rerun() # Trigger summarization


# --- Main Summarization Processing: Step 3 - Summarize Text ---
if st.session_state.get('file_to_summarize', None) is not None and st.session_state.file_to_summarize['id'] not in st.session_state.processed_file_ids:

    file_info_to_process = st.session_state.file_to_summarize
    file_id_to_process = file_info_to_process['id']
    filename_to_process = file_info_to_process['name']
    file_content_to_process = file_info_to_process['content']

    st.session_state.file_to_summarize = None

    logging.info(f"Starting summarization: {filename_to_process} (ID: {file_id_to_process}).")

    with st.spinner(f"'{filename_to_process}' ì²˜ë¦¬ ë° ìš”ì•½ ì¤‘..."):
        tokenizer = get_tokenizer()
        summary, summary_error = summarize_document(file_content_to_process, filename_to_process, MODEL, tokenizer)

        if summary_error:
             st.warning(f"'{filename_to_process}' ìš”ì•½ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ:\n{summary_error}")

        st.session_state.doc_summaries[filename_to_process] = summary
        st.session_state.processed_file_ids.add(file_id_to_process)

    st.success(f"ğŸ“„ '{filename_to_process}' ì—…ë¡œë“œ ë° ìš”ì•½ ì™„ë£Œ! ìš”ì•½ ë‚´ìš©ì´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë©ë‹ˆë‹¤.")
    logging.info(f"Successfully processed and summarized: {filename_to_process}.")
    st.rerun()


# Display summary expander
if st.session_state.doc_summaries:
    with st.expander("ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ ë³´ê¸°", expanded=False):
        for fname in sorted(st.session_state.doc_summaries.keys()):
             summ = st.session_state.doc_summaries[fname]
             st.text_area(f"ìš”ì•½: {fname}", summ, height=150, key=f"summary_display_{fname}", disabled=True)
        if st.button("ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ê¸°", key="clear_doc_summaries_btn_exp"):
             st.session_state.doc_summaries = {}
             st.session_state.file_to_summarize = None
             st.session_state.file_info_to_process_safely_captured = None
             st.session_state.uploaded_image_for_next_prompt = None
             logging.info("Document summaries cleared.")
             st.rerun()


# Display chat history
st.markdown("---")
st.subheader("ëŒ€í™”")

msgs_to_display = [msg for msg in st.session_state.messages if msg['role'] != 'system']

for message in msgs_to_display:
    with st.chat_message(message["role"]):
        content = message["content"]
        if isinstance(content, str):
            st.markdown(content)
        elif isinstance(content, list):
            for part in content:
                if part.get("type") == "text" and "text" in part:
                    st.markdown(part["text"])
                elif part.get("type") == "image_url" and "image_url" in part and "url" in part["image_url"]:
                      try:
                          image_url = part["image_url"]["url"]
                          header, base64_data = image_url.split(',')
                          image_bytes = base64.b64decode(base64_data)
                          image_type = header.split(':')[1].split(';')[0] if ':' in header and ';' in header else 'image/png'
                          st.image(image_bytes, use_column_width=True)
                      except Exception as e:
                           logging.error(f"Error displaying image from multimodal message in history: {e}", exc_info=True)
                           st.warning("âš ï¸ ì´ë¯¸ì§€ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# ------------------------------------------------------------------
# CHAT INPUT & RESPONSE GENERATION (with Streaming)
# ------------------------------------------------------------------
if prompt := st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    user_message_content: Any

    if st.session_state.get('uploaded_image_for_next_prompt', None) is not None:
        image_info = st.session_state.uploaded_image_for_next_prompt
        logging.info(f"Combining image '{image_info['name']}' (ID: {image_info['id']}) with next prompt.")

        base64_image = base64.b64encode(image_info['bytes']).decode('utf-8')
        image_url = f"data:{image_info['type']};base64,{base64_image}"

        user_message_content = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": image_url}}
        ]

        st.session_state.uploaded_image_for_next_prompt = None

    else:
        user_message_content = prompt

    st.session_state.messages.append({'role': 'user', 'content': user_message_content})

    # Display the new user message in the chat history area
    with st.chat_message("user"):
        content = user_message_content
        if isinstance(content, str):
            st.markdown(content)
        elif isinstance(content, list):
             for part in content:
                 if part.get("type") == "text" and "text" in part:
                     st.markdown(part["text"])
                 elif part.get("type") == "image_url" and "image_url" in part and "url" in part["image_url"]:
                      # Re-display the image part
                      try:
                          image_url = part["image_url"]["url"]
                          header, base64_data = image_url.split(',')
                          image_bytes = base64.b64decode(base64_data)
                          image_type = header.split(':')[1].split(';')[0] if ':' in header and ';' in header else 'image/png'
                          st.image(image_bytes, use_column_width=True)
                      except Exception as e:
                           logging.error(f"Error displaying image from multimodal message: {e}", exc_info=True)
                           st.warning("âš ï¸ ì´ë¯¸ì§€ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    # --- Context Building ---
    # Flag to control if API call proceeds
    should_proceed_with_api_call = True

    try:
        tokenizer = get_tokenizer()
        current_model_max_tokens = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

        current_system_prompt = st.session_state.messages[0] if 'messages' in st.session_state and st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT

        base_tokens = num_tokens_from_messages([current_system_prompt], tokenizer)
        available_tokens_for_context = current_model_max_tokens - base_tokens - RESERVED_TOKENS

        if available_tokens_for_context <= 0:
             st.error("ì„¤ì •ëœ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì˜ˆì•½ í† í°ìœ¼ë¡œ ì¸í•´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ì˜ˆì•½ í† í° ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
             logging.error("Not enough tokens for context after system prompt and reserved.")
             should_proceed_with_api_call = False # Set flag to False

        # If we should not proceed, skip context building other than system prompt
        if should_proceed_with_api_call:
            conversation_context = [current_system_prompt]

            doc_summary_context = []
            for fname in sorted(st.session_state.doc_summaries.keys()):
                summ = st.session_state.doc_summaries[fname]
                summary_msg = {'role': 'system', 'content': f"[ë¬¸ì„œ '{fname}' ìš”ì•½ ì°¸ê³ ]\n{summ}"}
                doc_summary_context.append(summary_msg)
            conversation_context.extend(doc_summary_context)

            history_messages = [msg for msg in st.session_state.messages[:-1] if msg['role'] != 'system']
            current_user_message = st.session_state.messages[-1]

            current_user_tokens_estimate = num_tokens_from_messages([current_user_message], tokenizer)
            base_context_tokens_estimate = num_tokens_from_messages(conversation_context, tokenizer)

            available_for_history_estimate = current_model_max_tokens - base_context_tokens_estimate - current_user_tokens_estimate - RESERVED_TOKENS

            history_context = []
            history_tokens_added_estimate = 0

            for msg in reversed(history_messages):
                msg_tokens_estimate = num_tokens_from_messages([msg], tokenizer)
                if available_for_history_estimate - (history_tokens_added_estimate + msg_tokens_estimate) >= 0:
                     history_context.insert(0, msg)
                     history_tokens_added_estimate += msg_tokens_estimate
                else:
                     logging.warning("Older chat history skipped due to estimated token limit.")
                     break

            conversation_context.extend(history_context)
            conversation_context.append(current_user_message)

            total_estimated_tokens = num_tokens_from_messages(conversation_context, tokenizer)
            logging.info(f"Estimated context tokens (text parts only): {total_estimated_tokens} for model {MODEL}.")

        else:
             # If should_proceed_with_api_call is False, set conversation_context
             # to minimal or empty so the API call check below also fails or shows error
             conversation_context = [current_system_prompt] # Minimal context

    except Exception as e:
        st.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error constructing conversation context: {e}", exc_info=True)
        should_proceed_with_api_call = False # Set flag to False

        # Fallback context if error occurred, but only if we were supposed to proceed
        # This fallback is now less critical as the flag controls flow.
        # Just ensure conversation_context is defined.
        current_system_prompt = st.session_state.messages[0] if 'messages' in st.session_state and st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT
        conversation_context = [current_system_prompt]
        last_user_msg = st.session_state.messages[-1] if 'messages' in st.session_state and st.session_state.messages else None
        if last_user_msg:
             conversation_context.append(last_user_msg)


    # --- API Call and Response Generation (with Streaming) ---
    # Only proceed if the flag is True and context is valid
    if should_proceed_with_api_call and conversation_context and any(msg['role'] != 'system' for msg in conversation_context):
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                stream = client.chat.completions.create(
                    model=MODEL,
                    messages=conversation_context,
                    stream=True,
                    temperature=0.75 if MODE == 'Poetic' else 0.4,
                    timeout=120
                )
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content is not None:
                        chunk_content = chunk.choices[0].delta.content
                        full_response += chunk_content
                        message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)
                logging.info(f"Assistant response received (length: {len(full_response)} chars).")

            except Exception as e:
                full_response = f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(full_response)
                logging.error(f"Error during OpenAI API call/streaming: {e}", exc_info=True)
                if "rate_limit_exceeded" in str(e).lower():
                     st.error("âš ï¸ Rate Limit Exceeded: OpenAI API ì‚¬ìš©ëŸ‰ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ëŒ€í™” ê¸¸ì´ë¥¼ ì¤„ì´ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                elif "context_window_exceeded" in str(e).lower():
                     st.error("âš ï¸ Context Window Exceeded: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ëª¨ë¸ì˜ ìµœëŒ€ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ëŒ€í™” ê¸¸ì´ë‚˜ ì—…ë¡œë“œëœ ë¬¸ì„œ ì–‘ì„ ì¤„ì´ì„¸ìš”.")

    else:
         # If flag is False or context is invalid
         # An error message is already displayed in the context building block if flag became False
         # This else block handles cases where context is invalid but no specific error was raised earlier
         if should_proceed_with_api_call: # If we *intended* to proceed but context became invalid late
              full_response = "âš ï¸ ì‘ë‹µ ìƒì„±ì— í•„ìš”í•œ ìœ íš¨í•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
              st.chat_message("assistant").error(full_response)
         else:
              # If should_proceed_with_api_call was already set to False, the error was handled in the try...except block
              pass


    if full_response:
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         save_history(HISTORY_FILE, st.session_state.messages)


# --- Footer or additional info ---
st.sidebar.markdown("---")
st.sidebar.caption("Liel Chatbot v1.7.1 (Syntax Error Fix)")