import json
import os
import streamlit as st
from openai import OpenAI
from pypdf import PdfReader
import docx
import pandas as pd
from time import sleep
import tiktoken
import logging
from typing import List, Dict, Tuple, Optional, Any
import io
import base64

# ------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------------------------------------------
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# ------------------------------------------------------------------
# CONFIGURATION & CONSTANTS
# ------------------------------------------------------------------
MODEL_CONTEXT_LIMITS = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4o": 128000,
    "gpt-4-turbo": 128000,
    "gpt-4o-mini": 128000, # gpt-4o-miniì˜ ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì¶”ê°€
}
# gpt-4o-mini ëª¨ë¸ í¬í•¨
MULTIMODAL_VISION_MODELS = ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"] # ë©€í‹°ëª¨ë‹¬ ì§€ì› ëª¨ë¸ ëª©ë¡

DEFAULT_ENCODING = "cl100k_base"
CHUNK_SIZE = 2000
RESERVED_TOKENS = 1500
HISTORY_FILE = "history/chat_history.json"
CHUNK_PROMPT_FOR_SUMMARY = 'Summarize the key points of this text chunk in 2-3 concise bullet points, focusing on the main information.'

# ------------------------------------------------------------------
# STREAMLIT PAGE SETUP
# ------------------------------------------------------------------
st.set_page_config(page_title="Liel â€“ AI Chatbot", layout="wide", initial_sidebar_state="auto")

# ------------------------------------------------------------------
# OPENAI CLIENT INITIALIZATION & API KEY HANDLING
# ------------------------------------------------------------------
def initialize_openai_client() -> Optional[OpenAI]:
    """Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = None
    try:
        if "general" in st.secrets and "OPENAI_API_KEY" in st.secrets["general"]:
            api_key = st.secrets["general"]["OPENAI_API_KEY"]
            logging.info("API Key loaded from Streamlit Secrets.")
        elif os.environ.get("OPENAI_API_KEY"):
            api_key = os.environ.get("OPENAI_API_KEY")
            logging.info("API Key loaded from environment variable.")

    except Exception as e:
        logging.warning(f"Error accessing API key from secrets or env: {e}")

    if not api_key or not api_key.startswith("sk-"):
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        st.warning("API í‚¤ë¥¼ `.streamlit/secrets.toml` íŒŒì¼ì— `[general]\nOPENAI_API_KEY='sk-...'` í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return None

    try:
        client = OpenAI(api_key=api_key)
        logging.info("OpenAI client initialized.")
        return client
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error(f"Client initialization failed: {e}", exc_info=True)
        return None

client = initialize_openai_client()

if client is None:
    st.stop() # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# ------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------
def get_tokenizer(encoding_name: str = DEFAULT_ENCODING) -> tiktoken.Encoding:
    """ì§€ì •ëœ ì´ë¦„ì˜ tiktoken ì¸ì½”ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return tiktoken.get_encoding(encoding_name)
    except ValueError:
        logging.warning(f"Encoding '{encoding_name}' not found. Using default '{DEFAULT_ENCODING}'.")
        return tiktoken.get_encoding(DEFAULT_ENCODING)

def num_tokens_from_string(string: str, encoding: tiktoken.Encoding) -> int:
    """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return len(encoding.encode(string)) if string else 0

def num_tokens_from_messages(messages: List[Dict[str, Any]], encoding: tiktoken.Encoding) -> int:
    """Calculates tokens for text parts in messages (simplified for multimodal)."""
    num_tokens = 0
    for message in messages:
        num_tokens += 4

        content = message.get("content")
        if isinstance(content, str):
             num_tokens += num_tokens_from_string(content, encoding)
        elif isinstance(content, list):
             for part in content:
                 if part.get("type") == "text" and "text" in part:
                      num_tokens += num_tokens_from_string(part["text"], encoding)

        if "name" in message:
            num_tokens -= 1

    num_tokens += 2
    return num_tokens


# read_file remains the same
@st.cache_data(show_spinner=False, hash_funcs={docx.document.Document: id, pd.DataFrame: pd.util.hash_pandas_object})
def read_file(uploaded_file_content_bytes, filename, file_type) -> Tuple[str, Optional[str]]:
    """ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë°”ì´íŠ¸ ë‚´ìš©ì„ ì½ì–´ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)."""
    try:
        logging.info(f"Reading file: {filename} (Type: {file_type})")
        file_like_object = io.BytesIO(uploaded_file_content_bytes)

        if file_type == 'text/plain':
            try:
                return file_like_object.read().decode('utf-8'), None
            except UnicodeDecodeError:
                file_like_object.seek(0)
                return file_like_object.read().decode('cp949'), None
        elif file_type == 'application/pdf':
            reader = PdfReader(file_like_object)
            text_parts = [page.extract_text() or '' for page in reader.pages]
            return '\n'.join(part for part in text_parts if part), None
        elif 'wordprocessingml.document' in file_type:
            doc = docx.Document(file_like_object)
            return '\n'.join(p.text for p in doc.paragraphs), None
        elif 'spreadsheetml.sheet' in file_type:
            df = pd.read_excel(file_like_object, engine='openpyxl')
            return df.to_csv(index=False, sep='\t'), None
        elif file_type in ['image/jpeg', 'image/png']:
             return '', f"ì´ë¯¸ì§€ íŒŒì¼ì€ í…ìŠ¤íŠ¸ ë³€í™˜ ì§€ì› ì•ˆ í•¨: {file_type}"
        else:
            return '', f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}"
    except Exception as e:
        logging.error(f"Error reading file {filename}: {e}", exc_info=True)
        return '', f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# summarize_document: _tokenizer parameter for caching
@st.cache_data(show_spinner=False)
def summarize_document(text: str, filename: str, model: str, _tokenizer: tiktoken.Encoding) -> Tuple[str, Optional[str]]:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ë¡œ ìš”ì•½."""
    if not text:
        return "(ë¬¸ì„œ ë‚´ìš© ì—†ìŒ)", None

    summaries = []
    chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]
    total_chunks = len(chunks)
    logging.info(f"Starting summarization for '{filename}' ({total_chunks} chunks).")

    progress_bar = st.progress(0, text=f"'{filename}' ìš”ì•½ ì¤‘... [0/{total_chunks}]")
    summary_errors = []

    for i, chunk in enumerate(chunks):
        progress = (i + 1) / total_chunks
        progress_bar.progress(progress, text=f"'{filename}' ìš”ì•½ ì¤‘... [{i+1}/{total_chunks}]")

        chunk_tokens = num_tokens_from_string(chunk, _tokenizer)
        model_limit = MODEL_CONTEXT_LIMITS.get(model, 8192)
        if chunk_tokens > model_limit - 500:
             logging.warning(f"Chunk {i+1} tokens ({chunk_tokens}) near/exceeds model limit ({model}).")

        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {'role': 'system', 'content': CHUNK_PROMPT_FOR_SUMMARY},
                    {'role': 'user', 'content': chunk}
                ],
                max_tokens=250,
                temperature=0.3,
                timeout=60
            )
            if response.choices and response.choices[0].message and response.choices[0].message.content:
                summaries.append(response.choices[0].message.content.strip())
            else:
                 logging.warning(f"Chunk {i+1} summarization returned no content.")
                 summaries.append(f"(ì²­í¬ {i+1} ìš”ì•½ ë‚´ìš© ì—†ìŒ)")

            sleep(0.15)
        except Exception as e:
            error_msg = f"ì²­í¬ {i+1} ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.warning(error_msg)
            logging.error(f"Error summarizing chunk {i+1}: {e}", exc_info=True)
            summaries.append(f"(ì²­í¬ {i+1} ìš”ì•½ ì‹¤íŒ¨)")
            summary_errors.append(error_msg)

    progress_bar.empty()
    return '\n'.join(summaries), "\n".join(summary_errors) if summary_errors else None

# load_history remains the same
@st.cache_data(show_spinner=False)
def load_history(path: str) -> List[Dict[str, str]]:
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        logging.info(f"History file not found at {path}.")
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            history = json.load(f)
            logging.info(f"Loaded {len(history)} messages from {path}.")
            return history
    except (json.JSONDecodeError, FileNotFoundError, OSError) as e:
        logging.warning(f"Error loading history file {path}: {e}")
        if os.path.exists(path):
             st.sidebar.warning(f"ëŒ€í™” ê¸°ë¡ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ìƒˆ ê¸°ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ({os.path.basename(path)})")
        return []
    except Exception as e:
        logging.error(f"Unexpected error loading history from {path}: {e}", exc_info=True)
        st.sidebar.error(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


# save_history remains the same (only saves text messages)
def save_history(path: str, msgs: List[Dict[str, Any]]):
    """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë° ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ì œì™¸)."""
    msgs_to_save = [msg for msg in msgs if msg['role'] != 'system' and isinstance(msg.get('content'), str)]

    if not msgs_to_save:
        if os.path.exists(path):
            try:
                os.remove(path)
                logging.info(f"History file {path} removed as messages are empty.")
            except OSError as e:
                logging.error(f"Failed to remove empty history file {path}: {e}")
        return

    history_dir = os.path.dirname(path)
    if history_dir:
        try:
            os.makedirs(history_dir, exist_ok=True)
        except Exception as e:
             logging.error(f"Error creating history directory {history_dir}: {e}", exc_info=True)
             st.sidebar.error(f"ê¸°ë¡ í´ë” ìƒì„± ì‹¤íŒ¨: {e}. ê¸°ë¡ ì €ì¥ì´ ì•ˆ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
             return

    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(msgs_to_save, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.error(f"Error saving history to {path}: {e}", exc_info=True)


# ------------------------------------------------------------------
# SESSION STATE INITIALIZATION
# ------------------------------------------------------------------
if 'messages' not in st.session_state:
    st.session_state.messages: List[Dict[str, Any]] = load_history(HISTORY_FILE)
    logging.info(f"Session initialized. Loaded {len(st.session_state.messages)} messages.")

if 'doc_summaries' not in st.session_state:
    st.session_state.doc_summaries: Dict[str, str] = {}

if 'processed_file_keys' not in st.session_state:
     st.session_state.processed_file_keys: set = set()

if 'file_to_summarize' not in st.session_state:
    st.session_state.file_to_summarize: Optional[Dict] = None

if 'file_info_to_process_safely_captured_by_key' not in st.session_state:
     st.session_state.file_info_to_process_safely_captured_by_key: Optional[Dict] = None

if 'uploaded_image_for_next_prompt' not in st.session_state:
    st.session_state.uploaded_image_for_next_prompt: Optional[Dict] = None


# ------------------------------------------------------------------
# SIDEBAR: MODEL, MODE SELECTION & OPTIONS
# ------------------------------------------------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

# gpt-4o-minië¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì„¤ì •
MODEL = st.sidebar.selectbox(
    'ëª¨ë¸ ì„ íƒ (ë©€í‹°ëª¨ë‹¬ ì§€ì›)',
    MULTIMODAL_VISION_MODELS,
    index=MULTIMODAL_VISION_MODELS.index("gpt-4o-mini") if "gpt-4o-mini" in MULTIMODAL_VISION_MODELS else 0 # gpt-4o-minië¥¼ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •
)
MAX_CONTEXT_TOKENS = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

MODE = st.sidebar.radio('ì‘ë‹µ ëª¨ë“œ', ('Poetic', 'Logical'), index=0, key='mode_selection')

st.sidebar.markdown("---")
st.sidebar.subheader("ê´€ë¦¬")

# build_full_session_content remains the same
def build_full_session_content() -> str:
    """ë¬¸ì„œ ìš”ì•½ê³¼ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•©ì³ í…ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    parts = []
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    parts.append(f"Liel Chat Session Content - {timestamp}")
    parts.append(f"Model: {MODEL}, Mode: {MODE}\n")

    if st.session_state.doc_summaries:
        parts.append("===== Uploaded Document Summaries =====")
        for fname in sorted(st.session_state.doc_summaries.keys()):
             summ = st.session_state.doc_summaries[fname]
             parts.append(f"\n--- Summary: {fname} ---")
             parts.append(summ)
             parts.append("-" * (len(fname) + 16))
        parts.append("\n" + "=" * 30 + "\n")

    parts.append("===== Conversation History =====")
    msgs_to_include = [msg for msg in st.session_state.messages if msg['role'] != 'system']
    if not msgs_to_include:
         parts.append("(No conversation yet)")
    else:
        for m in msgs_to_include:
            role_icon = "ğŸ‘¤ User" if m['role'] == 'user' else "ğŸ¤– Liel"
            parts.append(f"\n{role_icon}:\n")
            content = m.get('content')
            if isinstance(content, str):
                 parts.append(content)
            elif isinstance(content, list):
                 multimodal_parts = []
                 for part in content:
                     if part.get("type") == "text" and "text" in part:
                         multimodal_parts.append(part["text"])
                     elif part.get("type") == "image_url" and "image_url" in part:
                         multimodal_parts.append("[Uploaded Image]")
                 parts.append("\n".join(multimodal_parts))
            else:
                 parts.append("(Unsupported message content format)")

            parts.append("-" * 20)

    return '\n'.join(parts)


# Download Button remains the same (no split functionality)
if [msg for msg in st.session_state.messages if msg['role'] != 'system'] or st.session_state.doc_summaries:
    session_content_txt = build_full_session_content()
    download_filename = f"liel_session_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.txt"
    st.sidebar.download_button(
        label="ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ",
        data=session_content_txt.encode('utf-8'),
        file_name=download_filename,
        mime='text/plain',
        help="ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ê³¼ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
    )

# --- Clear Button: Removed ---


# ------------------------------------------------------------------
# SYSTEM PROMPT DEFINITION
# ------------------------------------------------------------------
SYSTEM_PROMPT_CONTENT = (
    'You are Liel, a poetic, emotionally intelligent chatbot with lyrical grace. Respond with warmth, creativity, and empathy. Use rich language and metaphors when appropriate.'
    if MODE == 'Poetic' else
    'You are Liel, a highly analytical assistant focused on logic and precision. Provide clear, structured, and concise answers. Use bullet points or numbered lists for clarity when needed.'
)
SYSTEM_PROMPT = {'role': 'system', 'content': SYSTEM_PROMPT_CONTENT}

if 'messages' in st.session_state:
     st.session_state.messages = [msg for msg in st.session_state.messages if msg['role'] != 'system']
     st.session_state.messages.insert(0, SYSTEM_PROMPT)


# ------------------------------------------------------------------
# UI HEADER
# ------------------------------------------------------------------
st.title(f'ğŸ’¬ Liel â€“ {MODE} Chatbot')
st.caption(
    "ê¸°ì–µê³¼ ê°ì •ìœ¼ë¡œ ë¹›ë‚˜ëŠ” ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if MODE == 'Poetic' else
    "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
)


# Display chat history first
st.markdown("---")
st.subheader("ëŒ€í™”")

msgs_to_display = [msg for msg in st.session_state.messages if msg['role'] != 'system']

for message in msgs_to_display:
    with st.chat_message(message["role"]):
        content = message["content"]
        if isinstance(content, str):
            st.markdown(content)
        elif isinstance(content, list):
            for part in content:
                if part.get("type") == "text" and "text" in part:
                    st.markdown(part["text"])
                elif part.get("type") == "image_url" and "image_url" in part and "url" in part["image_url"]:
                      try:
                          image_url = part["image_url"]["url"]
                          header, base64_data = image_url.split(',')
                          image_bytes = base64.b64decode(base64_data)
                          image_type = header.split(':')[1].split(';')[0] if ':' in header and ';' in header else 'image/png'
                          st.image(image_bytes, use_container_width=True)
                      except Exception as e:
                           logging.error(f"Error displaying image from multimodal message in history: {e}", exc_info=True)
                           st.warning("âš ï¸ ì´ë¯¸ì§€ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# --- File Upload UI (Moved to Bottom) ---
uploaded_file = st.file_uploader(
    'íŒŒì¼ ì—…ë¡œë“œ (txt, pdf, docx, xlsx, jpg, png)',
    type=['txt', 'pdf', 'docx', 'xlsx', 'jpg', 'png'],
    key="file_uploader",
    help="í…ìŠ¤íŠ¸, PDF, ì›Œë“œ, ì—‘ì…€ íŒŒì¼ì€ ìš”ì•½í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ê³ , ì´ë¯¸ì§€ íŒŒì¼ì€ ë‹¤ìŒ ì§ˆë¬¸ê³¼ í•¨ê»˜ ëª¨ë¸ì—ê²Œ ì „ì†¡í•©ë‹ˆë‹¤."
)

# --- File Upload Handling and Queuing: Step 1 - Safely Capture File Info ---
# Logic for Step 1 remains the same, but its execution position moved here
logging.info(f"--- Start Streamlit Rerun ---")
logging.info(f"Uploaded file state: {uploaded_file is not None}")

if uploaded_file is not None:
    logging.info(f"Step 1: uploaded_file is NOT None. Processing potential file.")

    try:
        file_name_now = uploaded_file.name
        file_size_now = uploaded_file.size
        file_type_now = uploaded_file.type
        file_bytes_now = uploaded_file.getvalue()

        file_simple_key = f"{file_name_now}_{file_size_now}"

        logging.info(f"Step 1: Successfully accessed file attributes for {file_name_now}. Simple Key: {file_simple_key}.")

        is_already_processed_by_key = file_simple_key in st.session_state.processed_file_keys
        is_already_safely_captured_by_key = st.session_state.get('file_info_to_process_safely_captured_by_key', None) is not None and st.session_state.file_info_to_process_safely_captured_by_key.get('simple_key') == file_simple_key
        is_current_image_for_prompt_by_key = st.session_state.get('uploaded_image_for_next_prompt', None) is not None and st.session_state.uploaded_image_for_next_prompt.get('simple_key') == file_simple_key


        logging.info(f"Step 1: File {file_name_now} state checks (by key): processed={is_already_processed_by_key}, safely_captured={is_already_safely_captured_by_key}, waiting_image={is_current_image_for_prompt_by_key}")


        if not is_already_processed_by_key and not is_already_safely_captured_by_key and not is_current_image_for_prompt_by_key:
            logging.info(f"Step 1: File {file_name_now} (Key: {file_simple_key}) is new/unhandled. Capturing details.")
            st.session_state.file_info_to_process_safely_captured_by_key = {
                'simple_key': file_simple_key,
                'name': file_name_now,
                'type': file_type_now,
                'bytes': file_bytes_now
            }
            logging.info(f"Step 1: Stored captured details for {file_name_now} by key. Triggering rerun.")
            st.rerun()

        else:
             logging.info(f"Step 1: File {file_name_now} (Key: {file_simple_key}) is already in a handled state. Skipping capture.")


    except AttributeError as e:
        logging.warning(f"Step 1: AttributeError caught during uploaded_file attribute access/getvalue. File object likely transient: {e}")
        pass
    except Exception as e:
         logging.error(f"Step 1: Unexpected error during uploaded_file access: {e}", exc_info=True)
         pass
else:
     logging.info("Step 1: uploaded_file is None.")


# --- File Upload Handling and Queuing: Step 2 - Process Captured Info ---
# Logic for Step 2 remains the same, but its execution position moved here
captured_info_by_key = st.session_state.get('file_info_to_process_safely_captured_by_key', None)

if captured_info_by_key is not None and captured_info_by_key['simple_key'] not in st.session_state.processed_file_keys:

    logging.info(f"Step 2: captured_info_by_key is NOT None and not fully processed. Processing {captured_info_by_key['name']} (Key: {captured_info_by_key['simple_key']}).")

    st.session_state.file_info_to_process_safely_captured_by_key = None
    logging.info("Step 2: Cleared file_info_to_process_safely_captured_by_key state.")

    file_info_to_process = captured_info_by_key

    if file_info_to_process['type'] in ['image/jpeg', 'image/png']:
         logging.info(f"Step 2: File {file_info_to_process['name']} is an image. Queuing for next prompt.")

         st.session_state.uploaded_image_for_next_prompt = {
             'simple_key': file_info_to_process['simple_key'],
             'name': file_info_to_process['name'],
             'type': file_info_to_process['type'],
             'bytes': file_info_to_process['bytes']
         }

         st.info(f"âœ¨ ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ ì§ˆë¬¸ê³¼ í•¨ê»˜ ëª¨ë¸ì—ê²Œ ì „ì†¡ë©ë‹ˆë‹¤.")
         st.session_state.processed_file_keys.add(file_info_to_process['simple_key'])

         logging.info(f"Step 2: Displaying image {file_info_to_process['name']} in chat message.")
         with st.chat_message("user"):
             st.image(file_info_to_process['bytes'], caption=f"ì—…ë¡œë“œëœ ì´ë¯¸ì§€: {file_info_to_process['name']}", use_container_width=True)

         logging.info("Step 2: Triggering rerun after queuing image.")
         st.rerun()

    else: # Handle text-based files: Read content and queue for summarization (Step 3)
        logging.info(f"Step 2: File {file_info_to_process['name']} is text-based. Reading content.")
        content_text, read_error = read_file(file_info_to_process['bytes'], file_info_to_process['name'], file_info_to_process['type'])

        if read_error:
            st.error(f"'{file_info_to_process['name']}' íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {read_error}")
            st.session_state.processed_file_keys.add(file_info_to_process['simple_key']) # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ í‚¤ ì¶”ê°€í•˜ì—¬ ë£¨í”„ ë°©ì§€
        elif not content_text:
            st.warning(f"'{file_info_to_process['name']}' íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            st.session_state.processed_file_keys.add(file_info_to_process['simple_key']) # ë‚´ìš© ì—†ì–´ë„ í‚¤ ì¶”ê°€í•˜ì—¬ ë£¨í”„ ë°©ì§€
        else:
            st.session_state.file_to_summarize = {
                'simple_key': file_info_to_process['simple_key'],
                'name': file_info_to_process['name'],
                'content': content_text
            }
            # í…ìŠ¤íŠ¸ íŒŒì¼ì„ Step 3ìœ¼ë¡œ ë„˜ê¸°ê¸° ìœ„í•´ íì— ë„£ì€ ì§í›„ processed_file_keysì— ì¶”ê°€
            st.session_state.processed_file_keys.add(file_info_to_process['simple_key'])
            logging.info(f"File '{file_info_to_process['name']}' text content queued for summarization (Step 3) and key added to processed.")
            logging.info("Step 2: Triggering rerun after queuing text for summarization.")
            st.rerun()

else:
    logging.info("Step 2: captured_info_by_key is None or already processed.")


# --- Main Summarization Processing: Step 3 - Summarize Text ---
# Logic for Step 3 remains the same, but its execution position moved here
if st.session_state.get('file_to_summarize', None) is not None and st.session_state.file_to_summarize['simple_key'] not in st.session_state.doc_summaries: # Check against doc_summaries keys if processed

    file_info_to_process = st.session_state.file_to_summarize
    file_simple_key_to_process = file_info_to_process['simple_key']
    filename_to_process = file_info_to_process['name']
    file_content_to_process = file_info_to_process['content']

    # Check processed_file_keys again here defensively, though ideally Step 2's fix prevents needing this
    if file_simple_key_to_process in st.session_state.processed_file_keys and file_simple_key_to_process not in st.session_state.doc_summaries:

        logging.info(f"Step 3: file_to_summarize is NOT None and key found in processed_file_keys but not in doc_summaries. Starting summarization for {filename_to_process} (Key: {file_simple_key_to_process}).")
        st.session_state.file_to_summarize = None # Clear this state now that we're processing it

        with st.spinner(f"'{filename_to_process}' ì²˜ë¦¬ ë° ìš”ì•½ ì¤‘..."):
            tokenizer = get_tokenizer();
            summary, summary_error = summarize_document(file_content_to_process, filename_to_process, MODEL, tokenizer)

            if summary_error:
                 st.warning(f"'{filename_to_process}' ìš”ì•½ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ:\n{summary_error}")

            st.session_state.doc_summaries[filename_to_process] = summary
            # The key was already added to processed_file_keys in Step 2.
            # We could add it again here, but it's redundant if the Step 2 fix works.
            # st.session_state.processed_file_keys.add(file_simple_key_to_process)

        st.success(f"ğŸ“„ '{filename_to_process}' ì—…ë¡œë“œ ë° ìš”ì•½ ì™„ë£Œ! ìš”ì•½ ë‚´ìš©ì´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë©ë‹ˆë‹¤.")
        logging.info(f"Successfully processed and summarized: {filename_to_process}.")
        logging.info("Step 3: Summarization finished. Next interaction will proceed.")
        # No explicit rerun needed after Step 3 if it successfully processes;
        # the next user interaction or file upload will trigger the next run.

    else:
         # This case should ideally not be reached if Step 2's fix prevents re-queuing unprocessed files,
         # or if the file was already fully processed (in doc_summaries).
         # If it is reached, it might indicate a state inconsistency.
         logging.info(f"Step 3: file_to_summarize state inconsistent or already summarized (Key: {file_simple_key_to_process}). Skipping summarization.")
         # Clear potentially stale file_to_summarize state if the key is already processed
         if file_simple_key_to_process in st.session_state.processed_file_keys:
              st.session_state.file_to_summarize = None
              logging.info("Step 3: Cleared stale file_to_summarize state.")


else:
    logging.info("Step 3: file_to_summarize is None or already processed.")


# Display summary expander (remains in its logical place after file processing)
if st.session_state.doc_summaries:
    with st.expander("ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ ë³´ê¸°", expanded=False):
        for fname in sorted(st.session_state.doc_summaries.keys()):
             summ = st.session_state.doc_summaries[fname]
             st.text_area(f"ìš”ì•½: {fname}", summ, height=150, key=f"summary_display_{fname}", disabled=True)
        # Clear Summaries button (Optional - uncomment if needed)
        # if st.button("ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ê¸°", key="clear_doc_summaries_btn_exp"):
        #      st.session_state.messages = [msg for msg in st.session_state.messages if msg['role'] == 'system']
        #      st.session_state.doc_summaries = {}
        #      st.session_state.processed_file_keys = set()
        #      st.session_state.file_to_summarize = None
        #      st.session_state.file_info_to_process_safely_captured_by_key = None
        #      st.session_state.uploaded_image_for_next_prompt = None
        #
        #      save_history(HISTORY_FILE, st.session_state.messages)
        #      logging.info("Document summaries and file processing state cleared by user.")
        #      st.rerun()


# ------------------------------------------------------------------
# CHAT INPUT & RESPONSE GENERATION (with Streaming)
# ------------------------------------------------------------------

# File upload logic and processing steps are now placed BEFORE the chat input.

if prompt := st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    logging.info(f"Chat input detected: '{prompt}'")
    user_message_content: Any = prompt

    if st.session_state.get('uploaded_image_for_next_prompt', None) is not None:
        image_info = st.session_state.uploaded_image_for_next_prompt
        logging.info(f"Combining image '{image_info.get('name', 'N/A')}' (Key: {image_info.get('simple_key', 'N/A')}) with current prompt.")

        try:
            base64_image = base64.b64encode(image_info['bytes']).decode('utf-8')
            image_url = f"data:{image_info['type']};base64,{base64_image}"

            user_message_content = [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": image_url}}
            ]
            logging.info(f"Constructed multimodal message content for image and prompt.")

        except Exception as e:
            logging.error(f"Error encoding or formatting image for multimodal message: {e}", exc_info=True)
            st.error("âš ï¸ ì´ë¯¸ì§€ë¥¼ ë©”ì‹œì§€ì— í¬í•¨í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            user_message_content = prompt


        st.session_state.uploaded_image_for_next_prompt = None
        logging.info("Cleared uploaded_image_for_next_prompt state.")

    st.session_state.messages.append({'role': 'user', 'content': user_message_content})
    logging.info("Added user message to session state.")

    with st.chat_message("user"):
        content = user_message_content
        if isinstance(content, str):
            st.markdown(content)
        elif isinstance(content, list):
             for part in content:
                 if part.get("type") == "text" and "text" in part:
                     st.markdown(part["text"])
                 elif part.get("type") == "image_url" and "image_url" in part and "url" in part["image_url"]:
                      try:
                          image_url = part["image_url"]["url"]
                          header, base64_data = image_url.split(',')
                          image_bytes = base64.b64decode(base64_data)
                          image_type = header.split(':')[1].split(';')[0] if ':' in header and ';' in header else 'image/png'
                          st.image(image_bytes, use_container_width=True)
                      except Exception as e:
                           logging.error(f"Error displaying image from multimodal message in chat area: {e}", exc_info=True)
                           st.warning("âš ï¸ ì´ë¯¸ì§€ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    # --- Context Building ---
    should_proceed_with_api_call = True
    conversation_context: List[Dict[str, Any]] = []

    try:
        tokenizer = get_tokenizer()
        current_model_max_tokens = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

        current_system_prompt = st.session_state.messages[0] if 'messages' in st.session_state and st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT

        base_tokens_estimate = num_tokens_from_messages([current_system_prompt], tokenizer)

        current_user_tokens_estimate = num_tokens_from_messages([st.session_state.messages[-1]], tokenizer)

        available_tokens_for_context_estimate = current_model_max_tokens - base_tokens_estimate - current_user_tokens_estimate - RESERVED_TOKENS

        if available_tokens_for_context_estimate <= 0:
             st.error("ì„¤ì •ëœ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì˜ˆì•½ í† í°ìœ¼ë¡œ ì¸í•´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ì˜ˆì•½ í† í° ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
             logging.error("Context window too small based on text token estimate.")
             should_proceed_with_api_call = False

        if should_proceed_with_api_call:
            conversation_context = [current_system_prompt]

            doc_summary_context = []
            doc_tokens_added_estimate = 0
            for fname in sorted(st.session_state.doc_summaries.keys()):
                summ = st.session_state.doc_summaries[fname]
                summary_msg = {'role': 'system', 'content': f"[ë¬¸ì„œ '{fname}' ìš”ì•½ ì°¸ê³ ]\n{summ}"}
                summary_tokens_estimate = num_tokens_from_messages([summary_msg], tokenizer)

                if available_tokens_for_context_estimate - (doc_tokens_added_estimate + summary_tokens_estimate) >= 0:
                    doc_summary_context.append(summary_msg)
                    doc_tokens_added_estimate += summary_tokens_estimate
                else:
                    logging.warning(f"Document summary '{fname}' skipped due to estimated token limit.")
                    break

            conversation_context.extend(doc_summary_context)

            history_messages = [msg for msg in st.session_state.messages[:-1] if msg['role'] != 'system']
            history_context = []
            history_tokens_added_estimate = 0

            for msg in reversed(history_messages):
                msg_tokens_estimate = num_tokens_from_messages([msg], tokenizer)
                if available_tokens_for_context_estimate - (doc_tokens_added_estimate + history_tokens_added_estimate + msg_tokens_estimate) >= 0:
                     history_context.insert(0, msg)
                     history_tokens_added_estimate += msg_tokens_estimate
                else:
                     logging.warning("Older chat history skipped due to estimated token limit.")
                     break

            conversation_context.extend(history_context)
            conversation_context.append(st.session_state.messages[-1])

            total_estimated_tokens = num_tokens_from_messages(conversation_context, tokenizer)
            logging.info(f"Estimated final context tokens (text only): {total_estimated_tokens} for model {MODEL}.")


    except Exception as e:
        st.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error constructing conversation context: {e}", exc_info=True)
        should_proceed_with_api_call = False

        current_system_prompt = st.session_state.messages[0] if 'messages' in st.session_state and st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT
        conversation_context = [current_system_prompt]
        if 'messages' in st.session_state and st.session_state.messages:
             conversation_context.append(st.session_state.messages[-1])


    # --- API Call and Response Generation (with Streaming) ---
    if should_proceed_with_api_call and conversation_context and any(msg['role'] != 'system' for msg in conversation_context):
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                logging.info(f"Calling OpenAI API with model {MODEL}.")
                stream = client.chat.completions.create(
                    model=MODEL,
                    messages=conversation_context,
                    stream=True,
                    temperature=0.75 if MODE == 'Poetic' else 0.4,
                    timeout=120
                )
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content is not None:
                        chunk_content = chunk.choices[0].delta.content
                        full_response += chunk_content
                        message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)
                logging.info(f"Assistant response received (length: {len(full_response)} chars).")

            except Exception as e:
                full_response = f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(full_response)
                logging.error(f"Error during OpenAI API call/streaming: {e}", exc_info=True)
                if "rate_limit_exceeded" in str(e).lower():
                     st.error("âš ï¸ Rate Limit Exceeded: OpenAI API ì‚¬ìš©ëŸ‰ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ëŒ€í™” ê¸¸ì´ë¥¼ ì¤„ì´ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                elif "context_window_exceeded" in str(e).lower():
                     st.error("âš ï¸ Context Window Exceeded: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ëª¨ë¸ì˜ ìµœëŒ€ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ëŒ€í™” ê¸¸ì´ë‚˜ ì—…ë¡œë“œëœ ë¬¸ì„œ ì–‘ì„ ì¤„ì´ì„¸ìš”.")


    else:
         if should_proceed_with_api_call:
              full_response = "âš ï¸ ì‘ë‹µ ìƒì„±ì— í•„ìš”í•œ ìœ íš¨í•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
              st.chat_message("assistant").error(full_response)
         else:
              pass


    if full_response:
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         save_history(HISTORY_FILE, st.session_state.messages)


# --- Footer or additional info ---
st.sidebar.markdown("---")
st.sidebar.caption("Liel Chatbot v1.7.11 (í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë£¨í”„ ìˆ˜ì •)") # ë²„ì „ ë° ìƒíƒœ ì—…ë°ì´íŠ¸