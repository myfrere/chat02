import json
import os
import streamlit as st
from openai import OpenAI
from pypdf import PdfReader
import docx
import pandas as pd
from time import sleep # sleep í•¨ìˆ˜ëŠ” summarize_documentì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
import tiktoken
import logging
from typing import List, Dict, Tuple, Optional
import io

# ------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------------------------------------------
logging.basicConfig(level=logging.INFO, # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (INFO, WARNING, ERROR ë“±)
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# ------------------------------------------------------------------
# CONFIGURATION & CONSTANTS
# ------------------------------------------------------------------
MODEL_CONTEXT_LIMITS = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4o": 128000,
}
DEFAULT_ENCODING = "cl100k_base"
CHUNK_SIZE = 2000 # summarize_document í•¨ìˆ˜ì—ì„œ ì‚¬ìš©
RESERVED_TOKENS = 1500
HISTORY_FILE = "chat_history.json"
CHUNK_PROMPT_FOR_SUMMARY = 'Summarize the key points of this text chunk in 2-3 concise bullet points, focusing on the main information.' # summarize_document í•¨ìˆ˜ì—ì„œ ì‚¬ìš©

# ------------------------------------------------------------------
# STREAMLIT PAGE SETUP
# ------------------------------------------------------------------
st.set_page_config(page_title="Liel â€“ AI Chatbot", layout="wide", initial_sidebar_state="auto")

# ------------------------------------------------------------------
# OPENAI CLIENT INITIALIZATION & API KEY HANDLING
# ------------------------------------------------------------------
def initialize_openai_client() -> Optional[OpenAI]:
    """Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = None
    # 1. Streamlit Secrets í™•ì¸ (í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ê¶Œì¥)
    try:
        if "general" in st.secrets and "OPENAI_API_KEY" in st.secrets["general"]:
            api_key = st.secrets["general"]["OPENAI_API_KEY"]
            logging.info("OpenAI API Key loaded from Streamlit Secrets.")
        else:
             logging.info("OpenAI API Key not found in Streamlit Secrets.")
    except Exception as e:
        # secrets ì†ì„± ì ‘ê·¼ ì‹œ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        logging.warning(f"Could not read Streamlit Secrets['general']['OPENAI_API_KEY']: {e}")

    # 2. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Secretsì— ì—†ê±°ë‚˜ ë¡œì»¬ ì‹¤í–‰ ì‹œ)
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
        if api_key:
            logging.info("OpenAI API Key loaded from environment variable.")
        else:
            logging.warning("OpenAI API Key not found in environment variables either.")

    if not api_key or not api_key.startswith("sk-"):
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        st.warning("API í‚¤ë¥¼ `.streamlit/secrets.toml` íŒŒì¼ì— `[general]\nOPENAI_API_KEY='sk-...'` í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return None

    try:
        client = OpenAI(api_key=api_key)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ì„ íƒ ì‚¬í•­, í‚¤ ìœ íš¨ì„± ê²€ì¦)
        # client.models.list()
        logging.info("OpenAI client initialized successfully.")
        return client
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error(f"OpenAI client initialization failed: {e}", exc_info=True)
        return None

client = initialize_openai_client()

if client is None:
    st.stop() # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# ------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------
# @st.cache_data # tiktoken ë¡œë”©ì€ ë¹ ë¥´ë¯€ë¡œ ìºì‹œ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ
def get_tokenizer(encoding_name: str = DEFAULT_ENCODING) -> tiktoken.Encoding:
    """ì§€ì •ëœ ì´ë¦„ì˜ tiktoken ì¸ì½”ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return tiktoken.get_encoding(encoding_name)
    except ValueError:
        logging.warning(f"Encoding '{encoding_name}' not found. Using default '{DEFAULT_ENCODING}'.")
        return tiktoken.get_encoding(DEFAULT_ENCODING)

def num_tokens_from_string(string: str, encoding: tiktoken.Encoding) -> int:
    """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not string:
        return 0
    return len(encoding.encode(string))

def num_tokens_from_messages(messages: List[Dict[str, str]], encoding: tiktoken.Encoding) -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. OpenAI Cookbookì˜ ê³„ì‚° ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤."""
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # ëª¨ë“  ë©”ì‹œì§€ëŠ” <im_start>{role/name}\n{content}<im_end>\n í¬ë§·ì„ ë”°ë¦„
        for key, value in message.items():
            num_tokens += num_tokens_from_string(value, encoding)
            if key == "name": # ì´ë¦„ì´ ìˆëŠ” ê²½ìš° ì—­í• ì´ ìƒëµë˜ì–´ 1 í† í° ì ˆì•½
                num_tokens -= 1
    num_tokens += 2 # ëª¨ë“  ì‘ë‹µì€ <im_start>assistant<im_sep>ìœ¼ë¡œ ì‹œì‘
    return num_tokens

# allow_output_mutation=True ëŠ” íŒŒì¼ ê°ì²´ì™€ ê°™ì€ ë³€ê²½ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ìºì‹œí•  ë•Œ í•„ìš”í•  ìˆ˜ ìˆìŒ
# ì´ í•¨ìˆ˜ëŠ” ì´ì œ uploaded_file ê°ì²´ ìì²´ê°€ ì•„ë‹Œ, ë°”ì´íŠ¸ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë°›ìŠµë‹ˆë‹¤.
@st.cache_data(show_spinner=False, hash_funcs={docx.document.Document: id, pd.DataFrame: pd.util.hash_pandas_object})
def read_file(uploaded_file_content_bytes, filename, file_type) -> Tuple[str, Optional[str]]:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ (bytes) ë°›ì•„ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (ë‚´ìš©, None), ì‹¤íŒ¨ ì‹œ ('', ì—ëŸ¬ ë©”ì‹œì§€) ë°˜í™˜
    """
    try:
        logging.info(f"Reading file content for: {filename} (Type: {file_type})")
        # BytesIOë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ë¥˜ ê°ì²´ì²˜ëŸ¼ ë‹¤ë£¹ë‹ˆë‹¤.
        file_like_object = io.BytesIO(uploaded_file_content_bytes)

        if file_type == 'text/plain':
            try:
                # BytesIOì—ì„œ read() í›„ decode
                content = file_like_object.read().decode('utf-8')
            except UnicodeDecodeError:
                logging.warning(f"UTF-8 decoding failed for {filename}, trying cp949.")
                # read()ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ë©´ ìŠ¤íŠ¸ë¦¼ì´ ëì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ seek(0)ìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.
                file_like_object.seek(0)
                content = file_like_object.read().decode('cp949')
            return content, None
        elif file_type == 'application/pdf':
            reader = PdfReader(file_like_object)
            text_parts = []
            for i, page in enumerate(reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)
                except Exception as page_err:
                    logging.warning(f"Error extracting text from page {i+1} of {filename}: {page_err}")
            return '\n'.join(text_parts), None
        elif 'wordprocessingml.document' in file_type:
            doc = docx.Document(file_like_object)
            return '\n'.join(p.text for p in doc.paragraphs), None
        elif 'spreadsheetml.sheet' in file_type:
            df = pd.read_excel(file_like_object, engine='openpyxl')
            return df.to_csv(index=False, sep='\t'), None
        else:
            logging.warning(f"Unsupported file type for reading: {file_type}")
            return '', f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}"
    except Exception as e:
        logging.error(f"Error reading file content for {filename}: {e}", exc_info=True)
        return '', f"íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

@st.cache_data(show_spinner=False)
def summarize_document(text: str, filename: str, model: str, tokenizer: tiktoken.Encoding) -> Tuple[str, Optional[str]]:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìš”ì•½í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (ìš”ì•½ ë‚´ìš©, None), ì‹¤íŒ¨ ì‹œ ('', ì—ëŸ¬ ë©”ì‹œì§€) ë°˜í™˜
    """
    if not text:
        return "(ë¬¸ì„œ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤)", None

    summaries = []
    # ê°„ë‹¨í•œ ì²­í¬ ë¶„í•  (í† í° ê³ ë ¤ ì•ˆí•¨, ë¬¸ì ê¸¸ì´ ê¸°ì¤€)
    # ë” ì •êµí•œ í† í° ê¸°ë°˜ ì²­í¬ ë¶„í• ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]
    total_chunks = len(chunks)
    logging.info(f"Starting summarization for '{filename}' with {total_chunks} chunks.")

    progress_text = f"'{filename}' ìš”ì•½ ì¤‘... (ì´ {total_chunks}ê°œ ì²­í¬)"
    progress_bar = st.progress(0, text=progress_text)
    summary_errors = []

    for i, chunk in enumerate(chunks):
        current_progress = (i + 1) / total_chunks
        progress_bar.progress(current_progress, text=f"{progress_text} [{i+1}/{total_chunks}]")

        # ì²­í¬ í† í° ìˆ˜ í™•ì¸ (ì •êµí•œ í† í° ë¶„í• ì´ ì•„ë‹ˆë¯€ë¡œ ì—¬ê¸°ì„œ ì²´í¬í•˜ì—¬ ê²½ê³ ë§Œ)
        chunk_tokens = num_tokens_from_string(chunk, tokenizer)
        # ìš”ì•½ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í•œë„ ê·¼ì²˜ë©´ ê²½ê³  (ê°„ë‹¨í™” ìœ„í•´ ì—¬ê¸°ì„œëŠ” MAX_CONTEXT_TOKENS ì‚¬ìš©)
        if chunk_tokens > MODEL_CONTEXT_LIMITS.get(model, 8192) - 500: # ëª¨ë¸ë³„ ì‹¤ì œ í•œë„ì™€ ì—¬ìœ  ê³µê°„ ê³ ë ¤
             warning_msg = f"ì²­í¬ {i+1}ì˜ í† í° ìˆ˜ ({chunk_tokens})ê°€ ëª¨ë¸({model}) í•œë„ì— ê·¼ì ‘í•˜ê±°ë‚˜ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì´ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
             # st.warning(warning_msg) # ë„ˆë¬´ ë§ì€ ê²½ê³  ë°©ì§€
             logging.warning(warning_msg)


        try:
            # API í˜¸ì¶œ
            response = client.chat.completions.create(
                model=model, # ìš”ì•½ì—ë„ ë™ì¼ ëª¨ë¸ ì‚¬ìš© (ë˜ëŠ” ë” ì €ë ´í•œ ëª¨ë¸ ì§€ì • ê°€ëŠ¥)
                messages=[
                    {'role': 'system', 'content': CHUNK_PROMPT_FOR_SUMMARY},
                    {'role': 'user', 'content': chunk}
                ],
                max_tokens=250, # ìš”ì•½ ê¸¸ì´ ì œí•œ
                temperature=0.3, # ë‚®ì€ ì˜¨ë„
                timeout=60 # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ)
            )
            summary_part = response.choices[0].message.content.strip()
            summaries.append(summary_part)
            sleep(0.15) # API ì†ë„ ì œí•œ ë°©ì§€ (ì•½ê°„ ì¦ê°€)
        except Exception as e:
            error_msg = f"ì²­í¬ {i+1} ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.warning(error_msg) # UIì— ê²½ê³  í‘œì‹œ
            logging.error(f"Error summarizing chunk {i+1} of {filename}: {e}", exc_info=True)
            summaries.append(f"(ì²­í¬ {i+1} ìš”ì•½ ì‹¤íŒ¨)")
            summary_errors.append(error_msg)

    progress_bar.empty() # ì§„í–‰ë¥  ë°” ìˆ¨ê¹€
    full_summary = '\n'.join(summaries)
    logging.info(f"Finished summarization for '{filename}'.")
    error_report = "\n".join(summary_errors) if summary_errors else None
    return full_summary, error_report


@st.cache_data(show_spinner=False)
def load_history(path: str) -> List[Dict[str, str]]:
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        logging.info(f"History file not found at {path}, starting new history.")
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            history = json.load(f)
            logging.info(f"Loaded {len(history)} messages from {path}.")
            return history
    except json.JSONDecodeError:
        logging.warning(f"History file {path} is corrupted or invalid. Backing up and starting new history.")
        try:
            import datetime
            backup_path = f"{path}.{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.bak"
            os.rename(path, backup_path)
            logging.info(f"Corrupted history file backed up to {backup_path}")
            st.sidebar.warning(f"ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì†ìƒë˜ì–´ ë°±ì—… í›„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤: {os.path.basename(backup_path)}")
        except OSError as e:
            logging.error(f"Failed to backup corrupted history file {path}: {e}")
            st.sidebar.error(f"ì†ìƒëœ ê¸°ë¡ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        logging.error(f"Error loading history from {path}: {e}", exc_info=True)
        st.sidebar.error(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def save_history(path: str, msgs: List[Dict[str, str]]):
    """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    msgs_to_save = [msg for msg in msgs if msg['role'] != 'system']
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (Streamlit Cloud / ì¼ë¶€ í™˜ê²½ ëŒ€ë¹„)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(msgs_to_save, f, ensure_ascii=False, indent=2)
        # logging.info(f"Saved {len(msgs_to_save)} messages to {path}.") # ë„ˆë¬´ ìì£¼ ë¡œê¹…ë  ìˆ˜ ìˆìŒ
    except Exception as e:
        # Streamlit Cloudì—ì„œ ì“°ê¸° ê¶Œí•œì´ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
        logging.error(f"Error saving history to {path}: {e}", exc_info=True)
        # st.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # ì‚¬ìš©ìì—ê²Œ ë„ˆë¬´ ìì£¼ ë³´ì¼ ìˆ˜ ìˆìŒ


# ------------------------------------------------------------------
# SESSION STATE INITIALIZATION
# ------------------------------------------------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”: í˜ì´ì§€ ë¡œë“œ ì‹œ ë”± í•œ ë²ˆ ì‹¤í–‰ë©ë‹ˆë‹¤.
if 'messages' not in st.session_state:
    # history ë¡œë“œ ì‹œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì œì™¸í•˜ê³  ë¡œë“œ
    st.session_state.messages: List[Dict[str, str]] = load_history(HISTORY_FILE)
    # history ë¡œë“œ í›„ í˜„ì¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ ëª©ë¡ì˜ ì²« ìš”ì†Œë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•´ì•¼ ì•± ì‹œì‘ ì‹œ í•­ìƒ ìµœì‹  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë©ë‹ˆë‹¤.
    # ë‹¨, ì´ë¯¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆë‹¤ë©´ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    # ì•„ë˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ë¡œì§ìœ¼ë¡œ ì´ë™

if 'doc_summaries' not in st.session_state:
    st.session_state.doc_summaries: Dict[str, str] = {}
if 'processed_file_ids' not in st.session_state:
    st.session_state.processed_file_ids: set = set()

# íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸°ì—´ ìƒíƒœ ì¶”ê°€
if 'file_to_summarize' not in st.session_state:
    st.session_state.file_to_summarize: Optional[Dict] = None

# ì•ˆì „í•˜ê²Œ ìº¡ì²˜ëœ íŒŒì¼ ì •ë³´ ì €ì¥ìš© ì„ì‹œ ë³€ìˆ˜ ì´ˆê¸°í™”
if 'file_info_to_process_safely_captured' not in st.session_state:
     st.session_state.file_info_to_process_safely_captured: Optional[Dict] = None


# ------------------------------------------------------------------
# SIDEBAR: MODEL, MODE SELECTION & OPTIONS
# ------------------------------------------------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

MODEL = st.sidebar.selectbox(
    'ëª¨ë¸ ì„ íƒ',
    list(MODEL_CONTEXT_LIMITS.keys()),
    index=list(MODEL_CONTEXT_LIMITS.keys()).index("gpt-4o") if "gpt-4o" in MODEL_CONTEXT_LIMITS else 0 # ê¸°ë³¸ê°’ gpt-4o ì‹œë„
)
# MAX_CONTEXT_TOKENS ë³€ìˆ˜ ì •ì˜ (MODELì— ë”°ë¼ ë‹¬ë¼ì§)
MAX_CONTEXT_TOKENS = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)


MODE = st.sidebar.radio('ì‘ë‹µ ëª¨ë“œ', ('Poetic', 'Logical'), index=0, key='mode_selection')


st.sidebar.markdown("---")
st.sidebar.subheader("ê´€ë¦¬")

# ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
def build_full_session_content() -> str:
    """ë¬¸ì„œ ìš”ì•½ê³¼ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•©ì³ í…ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    parts = []
    # pd.Timestamp ì‚¬ìš© ì „ì— pandasê°€ ì„í¬íŠ¸ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    parts.append(f"Liel Chat Session Content - {timestamp}")
    parts.append(f"Model: {MODEL}, Mode: {MODE}\n")

    if st.session_state.doc_summaries:
        parts.append("===== Uploaded Document Summaries =====")
        for fname, summ in st.session_state.doc_summaries.items():
            parts.append(f"\n--- Summary: {fname} ---")
            parts.append(summ)
            parts.append("-" * (len(fname) + 16))
        parts.append("\n" + "=" * 30 + "\n")

    parts.append("===== Conversation History =====")
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ë‹¤ìš´ë¡œë“œ ë‚´ìš©ì— í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (save_historyì™€ ì¼ê´€ì„± ìœ ì§€)
    msgs_to_include = [msg for msg in st.session_state.messages if msg['role'] != 'system']
    if not msgs_to_include:
         parts.append("(No conversation yet)")
    else:
        for m in msgs_to_include:
            role_icon = "ğŸ‘¤ User" if m['role'] == 'user' else "ğŸ¤– Liel"
            parts.append(f"\n{role_icon}:\n{m['content']}")
            parts.append("-" * 20)

    return '\n'.join(parts)

# ëŒ€í™”ë‚˜ ë¬¸ì„œ ìš”ì•½(ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)ì´ ìˆì„ ë•Œë§Œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
# messagesì— ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸
if [msg for msg in st.session_state.messages if msg['role'] != 'system'] or st.session_state.doc_summaries:
    session_content_txt = build_full_session_content()
    download_filename = f"liel_session_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.txt"
    st.sidebar.download_button(
        label="ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ", # ë²„íŠ¼ ë ˆì´ë¸” ë³€ê²½
        data=session_content_txt.encode('utf-8'), # UTF-8 ì¸ì½”ë”© ëª…ì‹œ
        file_name=download_filename,
        mime='text/plain',
        help="ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ê³¼ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤." # ë„ì›€ë§ ë³€ê²½
    )

# í•„ìš”í•˜ë‹¤ë©´ ì´ˆê¸°í™” ë²„íŠ¼ì„ ë‹¤ì‹œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
if st.sidebar.button("ğŸ”„ ëŒ€í™” ë° ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.doc_summaries = {}
    st.session_state.processed_file_ids = set()
    st.session_state.file_to_summarize = None # ì²˜ë¦¬ ëŒ€ê¸° íŒŒì¼ë„ ì´ˆê¸°í™”
    st.session_state.file_info_to_process_safely_captured = None # ì•ˆì „ ìº¡ì²˜ëœ ì •ë³´ë„ ì´ˆê¸°í™”

    # chat_history.json íŒŒì¼ ì‚­ì œ
    if os.path.exists(HISTORY_FILE):
        try:
            os.remove(HISTORY_FILE)
            logging.info(f"History file {HISTORY_FILE} removed.")
            st.sidebar.success("ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except OSError as e:
            st.sidebar.error(f"ê¸°ë¡ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            logging.error(f"Failed to remove history file {HISTORY_FILE}: {e}")
    st.rerun()


# ------------------------------------------------------------------
# SYSTEM PROMPT DEFINITION
# ------------------------------------------------------------------
SYSTEM_PROMPT_CONTENT = (
    'You are Liel, a poetic, emotionally intelligent chatbot with lyrical grace. Respond with warmth, creativity, and empathy. Use rich language and metaphors when appropriate.'
    if MODE == 'Poetic' else
    'You are Liel, a highly analytical assistant focused on logic and precision. Provide clear, structured, and concise answers. Use bullet points or numbered lists for clarity when needed.'
)
SYSTEM_PROMPT = {'role': 'system', 'content': SYSTEM_PROMPT_CONTENT}

# ë§¤ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ í˜„ì¬ SYSTEM_PROMPTë¥¼ ë©”ì‹œì§€ ëª©ë¡ì˜ ì²« ìš”ì†Œë¡œ ê´€ë¦¬
# 'messages' ìƒíƒœê°€ ì¡´ì¬í•˜ê³  (ì´ˆê¸°í™” í›„), ì²« ìš”ì†Œê°€ í˜„ì¬ SYSTEM_PROMPTì™€ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸
if 'messages' in st.session_state:
     # ê¸°ì¡´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±° (í˜¹ì‹œ ì¤‘ë³µë˜ê±°ë‚˜ ì´ì „ ëª¨ë“œì˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´)
     st.session_state.messages = [msg for msg in st.session_state.messages if msg['role'] != 'system']
     # í˜„ì¬ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ëª©ë¡ ë§¨ ì•ì— ì¶”ê°€
     st.session_state.messages.insert(0, SYSTEM_PROMPT)
     # save_history í•¨ìˆ˜ì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì €ì¥ ì‹œ ì œì™¸ë©ë‹ˆë‹¤.


# ------------------------------------------------------------------
# UI HEADER
# ------------------------------------------------------------------
st.title(f'ğŸ’¬ Liel â€“ {MODE} Chatbot')
st.caption(
    "ê¸°ì–µê³¼ ê°ì •ìœ¼ë¡œ ë¹›ë‚˜ëŠ” ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if MODE == 'Poetic' else
    "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
)

# ------------------------------------------------------------------
# FILE UPLOAD UI
# ------------------------------------------------------------------
uploaded_file = st.file_uploader(
    'íŒŒì¼ ì—…ë¡œë“œ (txt, pdf, docx, xlsx)',
    type=['txt', 'pdf', 'docx', 'xlsx'],
    key="file_uploader",
    help="í…ìŠ¤íŠ¸, PDF, ì›Œë“œ, ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤."
)

# --- File Upload Handling and Queuing: Step 1 - Safely Capture File Info ---
# ì´ ë¸”ë¡ì€ Streamlit ì¬ì‹¤í–‰ ì‹œë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.
# íŒŒì¼ ì—…ë¡œë” ìœ„ì ¯ì— ìƒˆë¡œìš´ íŒŒì¼ ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìˆ˜ ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ìº¡ì²˜í•©ë‹ˆë‹¤.
if uploaded_file is not None:
    try:
        # íŒŒì¼ ê°ì²´ì˜ ê³ ìœ  ID, ì´ë¦„, íƒ€ì…, ë‚´ìš©(ë°”ì´íŠ¸)ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì´ ê³¼ì •ì—ì„œ AttributeErrorê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        file_id_now = uploaded_file.id # ì´ ë¼ì¸ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì—ˆìŠµë‹ˆë‹¤.
        file_name_now = uploaded_file.name
        file_type_now = uploaded_file.type
        file_bytes_now = uploaded_file.getvalue() # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´íŠ¸ë¡œ ì¦‰ì‹œ ê°€ì ¸ì˜µë‹ˆë‹¤.

        # ì •ë³´ ìº¡ì²˜ ì„±ê³µ ì‹œ:
        # ì´ íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆê±°ë‚˜, ì´ë¯¸ ì²˜ë¦¬ ëŒ€ê¸°ì—´ì— ìˆê±°ë‚˜, ì´ë¯¸ ì•ˆì „í•˜ê²Œ ìº¡ì²˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        is_already_processed = file_id_now in st.session_state.processed_file_ids
        is_already_in_main_queue = ('file_to_summarize' in st.session_state and \
                               st.session_state.file_to_summarize is not None and \
                               st.session_state.file_to_summarize['id'] == file_id_now)
        is_already_safely_captured = ('file_info_to_process_safely_captured' in st.session_state and \
                                      st.session_state.file_info_to_process_safely_captured is not None and \
                                      st.session_state.file_info_to_process_safely_captured['id'] == file_id_now)

        # ìƒˆë¡œìš´ íŒŒì¼ì´ë¼ë©´ (ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ê³ , íë‚˜ ìº¡ì²˜ ë³€ìˆ˜ì— ì—†ë‹¤ë©´):
        if not is_already_processed and not is_already_in_main_queue and not is_already_safely_captured:
            logging.info(f"Detected new file and attempting to safely capture details: {file_name_now} (ID: {file_id_now})")
            # ì•ˆì „í•˜ê²Œ ìº¡ì²˜ëœ ì •ë³´ë¥¼ ì„¸ì…˜ ìƒíƒœ ì„ì‹œ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.
            st.session_state.file_info_to_process_safely_captured = {
                'id': file_id_now,
                'name': file_name_now,
                'type': file_type_now,
                'bytes': file_bytes_now # ë°”ì´íŠ¸ ë‚´ìš© ì €ì¥
            }
            # ì •ë³´ë¥¼ ìº¡ì²˜í–ˆìœ¼ë‹ˆ ë‹¤ìŒ ë‹¨ê³„(ë°”ì´íŠ¸ -> í…ìŠ¤íŠ¸ ë³€í™˜)ë¡œ ì´ë™í•˜ê¸° ìœ„í•´ Streamlit ì¬ì‹¤í–‰
            st.rerun()
        # ë§Œì•½ ì´ë¯¸ ì•ˆì „í•˜ê²Œ ìº¡ì²˜ëœ íŒŒì¼ì´ë¼ë©´, ì„ì‹œ ìº¡ì²˜ ìƒíƒœë¥¼ ë¹„ì›Œ ë‹¤ìŒ ìº¡ì²˜ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        # (ì´ì „ ì¬ì‹¤í–‰ì—ì„œ ì´ë¯¸ ìº¡ì²˜ë˜ì—ˆì§€ë§Œ ì•„ì§ í•˜ìœ„ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì§€ ì•Šì€ ê²½ìš°)
        elif is_already_safely_captured:
             # ì„ì‹œ ìº¡ì²˜ ë³€ìˆ˜ ë¹„ìš°ê¸°
             st.session_state.file_info_to_process_safely_captured = None
             # ì´ ê²½ìš°ëŠ” ì´ë¯¸ ìº¡ì²˜ëœ ì •ë³´ê°€ í•˜ìœ„ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì„œ ì²˜ë¦¬ë  ê²ƒì´ë¯€ë¡œ ë³„ë„ì˜ rerunì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
             pass


    except AttributeError as e:
        # uploaded_fileì´ Noneì´ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì„ ë•Œ .id, .name ë“±ì— ì ‘ê·¼í•´ì„œ ë°œìƒí•˜ëŠ” ì—ëŸ¬ë¥¼ ì¡ìŠµë‹ˆë‹¤.
        # ì´ëŠ” ì •ìƒì ì¸ Streamlit ì¬ì‹¤í–‰ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” í˜„ìƒì…ë‹ˆë‹¤.
        logging.warning(f"AttributeError caught during uploaded_file attribute access (likely stale object in rerun): {e}")
        # ì´ ê²½ìš° í•´ë‹¹ ì¬ì‹¤í–‰ ì£¼ê¸°ì—ì„œëŠ” íŒŒì¼ ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ìº¡ì²˜í•˜ì§€ ì•Šê³  ê±´ë„ˆëœë‹ˆë‹¤.
        # ìœ íš¨í•œ uploaded_file ê°ì²´ê°€ ìˆëŠ” ë‹¤ìŒ ì¬ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ë‹¤ì‹œ ì‹œë„ë  ê²ƒì…ë‹ˆë‹¤.
        pass
    except Exception as e:
         # ê·¸ ì™¸ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ë¥¼ ì¡ìŠµë‹ˆë‹¤.
         logging.error(f"Unexpected error during uploaded_file attribute access: {e}", exc_info=True)
         pass


# --- File Upload Handling and Queuing: Step 2 - Convert Bytes to Text ---
# ì´ ë¸”ë¡ì€ ì•ˆì „í•˜ê²Œ ìº¡ì²˜ëœ íŒŒì¼ ì •ë³´(ë°”ì´íŠ¸)ê°€ ì„¸ì…˜ ìƒíƒœì— ìˆì„ ë•Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
# ë°”ì´íŠ¸ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ë©”ì¸ ì²˜ë¦¬ íì— ì¶”ê°€í•©ë‹ˆë‹¤.
if 'file_info_to_process_safely_captured' in st.session_state and \
   st.session_state.file_info_to_process_safely_captured is not None:

    file_info_captured = st.session_state.file_info_to_process_safely_captured

    # ì´ íŒŒì¼ IDê°€ ì´ë¯¸ ìµœì¢… ì²˜ë¦¬ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì§„í–‰
    if file_info_captured['id'] not in st.session_state.processed_file_ids:

        logging.info(f"Processing safely captured file info (bytes to text) for '{file_info_captured['name']}' (ID: {file_info_captured['id']}).")

        # ì•ˆì „í•˜ê²Œ ìº¡ì²˜ëœ ìƒíƒœë¥¼ ë¹„ì›Œì„œ ì´ ë‹¨ê³„ê°€ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        st.session_state.file_info_to_process_safely_captured = None

        # read_file í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ì´íŠ¸ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        content_text, read_error = read_file(file_info_captured['bytes'], file_info_captured['name'], file_info_captured['type'])

        if read_error:
            st.error(f"'{file_info_captured['name']}' íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {read_error}")
            # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ë„ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œí•˜ì—¬ ë¬´í•œ ë£¨í”„ ë°©ì§€
            st.session_state.processed_file_ids.add(file_info_captured['id'])
        elif not content_text:
            st.warning(f"'{file_info_captured['name']}' íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            st.session_state.processed_file_ids.add(file_info_captured['id']) # ë¹ˆ íŒŒì¼ë„ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œ
        else:
            # í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš° ë©”ì¸ ìš”ì•½ ì²˜ë¦¬ íì— ì¶”ê°€í•©ë‹ˆë‹¤.
            st.session_state.file_to_summarize = {
                'id': file_info_captured['id'],
                'name': file_info_captured['name'],
                'content': content_text # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
            }
            logging.info(f"File '{file_info_captured['name']}' text content queued for summarization.")
            # ìš”ì•½ ì²˜ë¦¬ ë‹¨ê³„ë¡œ ì´ë™í•˜ê¸° ìœ„í•´ Streamlit ì¬ì‹¤í–‰
            st.rerun()


# --- Main Summarization Processing: Step 3 - Summarize Text ---
# ì´ ë¸”ë¡ì€ ë©”ì¸ ì²˜ë¦¬ í(file_to_summarize - í…ìŠ¤íŠ¸ ë‚´ìš© í¬í•¨)ì— íŒŒì¼ì´ ìˆì„ ë•Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
if 'file_to_summarize' in st.session_state and \
   st.session_state.file_to_summarize is not None and \
   st.session_state.file_to_summarize['id'] not in st.session_state.processed_file_ids: # ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°

    file_info_to_process = st.session_state.file_to_summarize
    file_id_to_process = file_info_to_process['id']
    filename_to_process = file_info_to_process['name']
    file_content_to_process = file_info_to_process['content'] # ì´ ì‹œì ì—ì„œ ì´ë¯¸ í…ìŠ¤íŠ¸ ë‚´ìš©

    # ë©”ì¸ ì²˜ë¦¬ í ìŠ¬ë¡¯ì„ ë¹„ì›Œì„œ ì´ ë‹¨ê³„ê°€ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    st.session_state.file_to_summarize = None

    logging.info(f"Starting summarization processing from queue: {filename_to_process} (ID: {file_id_to_process})")

    with st.spinner(f"'{filename_to_process}' ì²˜ë¦¬ ë° ìš”ì•½ ì¤‘..."):
        tokenizer = get_tokenizer()
        # summarize_document í•¨ìˆ˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°”ë¡œ ë°›ìŠµë‹ˆë‹¤.
        # ëª¨ë¸ì€ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ í˜„ì¬ MODEL ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        summary, summary_error = summarize_document(file_content_to_process, filename_to_process, MODEL, tokenizer)

        if summary_error:
             st.warning(f"'{filename_to_process}' ìš”ì•½ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ:\n{summary_error}")

        st.session_state.doc_summaries[filename_to_process] = summary
        st.session_state.processed_file_ids.add(file_id_to_process) # ìµœì¢… ì²˜ë¦¬ ì™„ë£Œ ID ì¶”ê°€

    st.success(f"ğŸ“„ '{filename_to_process}' ì—…ë¡œë“œ ë° ìš”ì•½ ì™„ë£Œ!")
    logging.info(f"Successfully processed and summarized file: {filename_to_process}")
    # ìš”ì•½ ì™„ë£Œ í›„ Streamlit ì¬ì‹¤í–‰í•˜ì—¬ UI (ìš”ì•½ Expander, ë²„íŠ¼ ê°€ì‹œì„± ë“±)ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    st.rerun()


# ìš”ì•½ëœ ë¬¸ì„œ í‘œì‹œ (Expander ì‚¬ìš©)
if st.session_state.doc_summaries:
    with st.expander("ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ ë³´ê¸°", expanded=False):
        for fname, summ in st.session_state.doc_summaries.items():
            st.text_area(f"ìš”ì•½: {fname}", summ, height=150, key=f"summary_display_{fname}", disabled=True)
        # í•„ìš”í•˜ë‹¤ë©´ ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ëŠ” ë²„íŠ¼ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if st.button("ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ê¸°", key="clear_doc_summaries_btn_exp"):
             st.session_state.doc_summaries = {}
             # processed_file_idsëŠ” ë¬¸ì„œ ìš”ì•½ë¿ ì•„ë‹ˆë¼ íŒŒì¼ ì½ê¸° ì„±ê³µ ì—¬ë¶€ ë“± ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¥¼
             # ì¶”ì í•˜ëŠ” ë° ì‚¬ìš©ë˜ë¯€ë¡œ, ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš¸ ë•ŒëŠ” processed_file_idsë¥¼ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜
             # ë¬¸ì„œ ìš”ì•½ ê´€ë ¨ IDë§Œ ë³„ë„ë¡œ ê´€ë¦¬í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ëª¨ë‘ ì§€ìš°ëŠ” ê²ƒìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
             st.session_state.processed_file_ids = set()
             # ì„¸ì…˜ ìƒíƒœì˜ íŒŒì¼ ì²˜ë¦¬ ê´€ë ¨ ì„ì‹œ ë³€ìˆ˜ë„ ì´ˆê¸°í™”
             st.session_state.file_to_summarize = None
             st.session_state.file_info_to_process_safely_captured = None
             logging.info("Document summaries cleared by user from expander button.")
             st.rerun()


# ------------------------------------------------------------------
# DISPLAY CHAT HISTORY
# ------------------------------------------------------------------
st.markdown("---")
st.subheader("ëŒ€í™”")

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ëŒ€í™” ëª©ë¡ì— í‘œì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
msgs_to_display = [msg for msg in st.session_state.messages if msg['role'] != 'system']

for message in msgs_to_display:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ------------------------------------------------------------------
# CHAT INPUT & RESPONSE GENERATION (with Streaming)
# ------------------------------------------------------------------
if prompt := st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ê¸°ë¡ (session_stateì— ì¶”ê°€)
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    st.session_state.messages.append({'role': 'user', 'content': prompt})

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ---
    try:
        tokenizer = get_tokenizer()
        current_model_max_tokens = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # st.session_state.messagesì˜ ì²« ìš”ì†Œê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
        # (SYSTEM_PROMPT ì •ì˜ ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë¡œì§ì— ë”°ë¼)
        current_system_prompt = st.session_state.messages[0] if st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì˜ˆì•½ í† í°ì„ ì œì™¸í•œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í† í°
        base_tokens = num_tokens_from_messages([current_system_prompt], tokenizer)
        available_tokens_for_context = current_model_max_tokens - base_tokens - RESERVED_TOKENS

        if available_tokens_for_context <= 0:
             st.error("ì„¤ì •ëœ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì˜ˆì•½ í† í°ìœ¼ë¡œ ì¸í•´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ì˜ˆì•½ í† í° ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
             logging.error("Not enough tokens for context construction after system prompt and reserved tokens.")
             # st.stop() # ì•± ì „ì²´ ì¤‘ì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ í‘œì‹œ
             raise ValueError("Context window too small") # ì˜¤ë¥˜ ë°œìƒì‹œì¼œ í•˜ìœ„ ë¡œì§ ì¤‘ë‹¨


        # ì‹¤ì œ API í˜¸ì¶œì— ì‚¬ìš©ë  ë©”ì‹œì§€ ëª©ë¡ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
        conversation_context = [current_system_prompt]
        tokens_used = base_tokens

        # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        doc_summary_context = []
        doc_tokens_added = 0
        for fname, summ in reversed(list(st.session_state.doc_summaries.items())):
            summary_msg = {'role': 'system', 'content': f"[ë¬¸ì„œ '{fname}' ìš”ì•½ ì°¸ê³ ]\n{summ}"}
            temp_context = [summary_msg]
            summary_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš© + ì¶”ê°€ë  ìš”ì•½)
            if available_tokens_for_context - (tokens_used - base_tokens + doc_tokens_added + summary_tokens) >= 0:
                doc_summary_context.insert(0, summary_msg)
                doc_tokens_added += summary_tokens
            else:
                logging.warning(f"Document summary '{fname}' skipped due to token limit.")
                break

        conversation_context.extend(doc_summary_context)
        tokens_used += doc_tokens_added


        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        history_context = []
        history_tokens_added = 0
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ê¸°ë¡ë§Œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        msgs_to_consider = [msg for msg in st.session_state.messages if msg['role'] != 'system']

        for msg in reversed(msgs_to_consider):
            temp_context = [msg]
            msg_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš©(ìš”ì•½í¬í•¨) - base + ì¶”ê°€ë  íˆìŠ¤í† ë¦¬)
            if available_tokens_for_context - ((tokens_used - base_tokens) + history_tokens_added + msg_tokens) >= 0:
                history_context.insert(0, msg)
                history_tokens_added += msg_tokens
            else:
                logging.warning("Older chat history skipped due to token limit.")
                break

        conversation_context.extend(history_context)
        tokens_used += history_tokens_added


        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ ë¡œê¹…
        logging.info(f"Context constructed with {tokens_used} tokens for model {MODEL}.")
        # logging.debug(f"Final conversation context: {conversation_context}") # í•„ìš”ì‹œ ìƒì„¸ ë¡œê¹…

    except Exception as e:
        st.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error constructing conversation context: {e}", exc_info=True)
        # st.stop() # ì•± ì „ì²´ ì¤‘ì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ í‘œì‹œ
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìµœì†Œí•œì˜ ì»¨í…ìŠ¤íŠ¸ë§Œ í¬í•¨í•˜ì—¬ API í˜¸ì¶œì„ ì‹œë„í•˜ê±°ë‚˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ ì¶œë ¥
        conversation_context = [current_system_prompt, {'role': 'user', 'content': prompt}] # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ëŠ” í•­ìƒ í¬í•¨

    # --- API í˜¸ì¶œ ë° ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ---
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ê±°ë‚˜, ì˜¤ë¥˜ ì²˜ë¦¬ í›„ ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì§„í–‰
    if conversation_context and any(msg['role'] != 'system' for msg in conversation_context): # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        with st.chat_message("assistant"):
            message_placeholder = st.empty() # ì‘ë‹µ í‘œì‹œ ì˜ì—­
            full_response = ""
            try:
                stream = client.chat.completions.create(
                    model=MODEL,
                    messages=conversation_context,
                    stream=True,
                    temperature=0.75 if MODE == 'Poetic' else 0.4, # ëª¨ë“œë³„ ì˜¨ë„ ì¡°ì ˆ
                    # max_tokens= # í•„ìš”ì‹œ ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ ì„¤ì • ê°€ëŠ¥
                    timeout=120 # ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ (ì´ˆ)
                )
                # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° í‘œì‹œ
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        chunk_content = chunk.choices[0].delta.content
                        full_response += chunk_content
                        message_placeholder.markdown(full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼

                message_placeholder.markdown(full_response) # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                logging.info(f"Assistant response received (length: {len(full_response)} chars).")

            except Exception as e:
                full_response = f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(full_response)
                logging.error(f"Error during OpenAI API call or streaming: {e}", exc_info=True)

    else:
         full_response = "âš ï¸ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨ë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
         st.chat_message("assistant").error(full_response)


    # ì‘ë‹µ ê¸°ë¡ ì €ì¥ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)
    if full_response and not full_response.startswith("âš ï¸"): # ì •ìƒ ì‘ë‹µë§Œ ì €ì¥ (API ì˜¤ë¥˜ ë©”ì‹œì§€ ì œì™¸)
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         save_history(HISTORY_FILE, st.session_state.messages)
    elif full_response.startswith("âš ï¸"): # ì˜¤ë¥˜ ë©”ì‹œì§€ë„ ëŒ€í™” ëª©ë¡ì—ëŠ” í¬í•¨ì‹œí‚¤ì§€ë§Œ íŒŒì¼ì—ëŠ” ì €ì¥ ì•ˆ í•¨
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         #save_history(HISTORY_FILE, st.session_state.messages) # ì˜¤ë¥˜ ë©”ì‹œì§€ëŠ” íŒŒì¼ì— ì €ì¥í•˜ì§€ ì•ŠìŒ


# --- Footer or additional info ---
st.sidebar.markdown("---")
st.sidebar.caption("Liel Chatbot v1.4") # ë²„ì „ ì—…ë°ì´íŠ¸