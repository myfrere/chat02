import json
import os
import streamlit as st
from openai import OpenAI
from pypdf import PdfReader
import docx
import pandas as pd
from time import sleep
import tiktoken
import logging
from typing import List, Dict, Tuple, Optional, Generator

# ------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ------------------------------------------------------------------
logging.basicConfig(level=logging.INFO, # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (INFO, WARNING, ERROR ë“±)
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# ------------------------------------------------------------------
# CONFIGURATION & CONSTANTS
# ------------------------------------------------------------------
MODEL_CONTEXT_LIMITS = {
    "gpt-3.5-turbo": 16385,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4o": 128000,
}
DEFAULT_ENCODING = "cl100k_base"
CHUNK_SIZE = 2000
RESERVED_TOKENS = 1500 # ìŠ¤íŠ¸ë¦¬ë° ë° ì¶”ê°€ ì˜¤ë²„í—¤ë“œ ê³ ë ¤í•˜ì—¬ ì•½ê°„ ëŠ˜ë¦¼
HISTORY_FILE = "chat_history.json"
CHUNK_PROMPT_FOR_SUMMARY = 'Summarize the key points of this text chunk in 2-3 concise bullet points, focusing on the main information.'

# ------------------------------------------------------------------
# STREAMLIT PAGE SETUP
# ------------------------------------------------------------------
st.set_page_config(page_title="Liel â€“ AI Chatbot", layout="wide", initial_sidebar_state="auto")

# ------------------------------------------------------------------
# OPENAI CLIENT INITIALIZATION & API KEY HANDLING
# ------------------------------------------------------------------
def initialize_openai_client() -> Optional[OpenAI]:
    """Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    api_key = None
    # 1. Streamlit Secrets í™•ì¸ (í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ê¶Œì¥)
    try:
        if "general" in st.secrets and "OPENAI_API_KEY" in st.secrets["general"]:
            api_key = st.secrets["general"]["OPENAI_API_KEY"]
            logging.info("OpenAI API Key loaded from Streamlit Secrets.")
        else:
             logging.info("OpenAI API Key not found in Streamlit Secrets.")
    except Exception as e:
        logging.warning(f"Could not read Streamlit Secrets: {e}")

    # 2. í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (Secretsì— ì—†ê±°ë‚˜ ë¡œì»¬ ì‹¤í–‰ ì‹œ)
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
        if api_key:
            logging.info("OpenAI API Key loaded from environment variable.")
        else:
            logging.warning("OpenAI API Key not found in environment variables either.")

    if not api_key or not api_key.startswith("sk-"):
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.warning("ë¡œì»¬ ê°œë°œ ì‹œ: `.env` íŒŒì¼ì— `OPENAI_API_KEY='sk-...'` í˜•ì‹ìœ¼ë¡œ í‚¤ë¥¼ ì €ì¥í•˜ê³  `python-dotenv` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return None

    try:
        client = OpenAI(api_key=api_key)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ì„ íƒ ì‚¬í•­, í‚¤ ìœ íš¨ì„± ê²€ì¦)
        # client.models.list()
        logging.info("OpenAI client initialized successfully.")
        return client
    except Exception as e:
        st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.error(f"OpenAI client initialization failed: {e}", exc_info=True)
        return None

client = initialize_openai_client()

if client is None:
    st.stop() # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì•± ì¤‘ì§€

# ------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------
# @st.cache_data # tiktoken ë¡œë”©ì€ ë¹ ë¥´ë¯€ë¡œ ìºì‹œ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ
def get_tokenizer(encoding_name: str = DEFAULT_ENCODING) -> tiktoken.Encoding:
    """ì§€ì •ëœ ì´ë¦„ì˜ tiktoken ì¸ì½”ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        return tiktoken.get_encoding(encoding_name)
    except ValueError:
        logging.warning(f"Encoding '{encoding_name}' not found. Using default '{DEFAULT_ENCODING}'.")
        return tiktoken.get_encoding(DEFAULT_ENCODING)

def num_tokens_from_string(string: str, encoding: tiktoken.Encoding) -> int:
    """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not string:
        return 0
    return len(encoding.encode(string))

def num_tokens_from_messages(messages: List[Dict[str, str]], encoding: tiktoken.Encoding) -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. OpenAI Cookbookì˜ ê³„ì‚° ë°©ì‹ì„ ë”°ë¦…ë‹ˆë‹¤."""
    num_tokens = 0
    for message in messages:
        num_tokens += 4  # ëª¨ë“  ë©”ì‹œì§€ëŠ” <im_start>{role/name}\n{content}<im_end>\n í¬ë§·ì„ ë”°ë¦„
        for key, value in message.items():
            num_tokens += num_tokens_from_string(value, encoding)
            if key == "name": # ì´ë¦„ì´ ìˆëŠ” ê²½ìš° ì—­í• ì´ ìƒëµë˜ì–´ 1 í† í° ì ˆì•½
                num_tokens -= 1
    num_tokens += 2 # ëª¨ë“  ì‘ë‹µì€ <im_start>assistant<im_sep>ìœ¼ë¡œ ì‹œì‘
    return num_tokens

# allow_output_mutation=True ëŠ” íŒŒì¼ ê°ì²´ì™€ ê°™ì€ ë³€ê²½ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ìºì‹œí•  ë•Œ í•„ìš”í•  ìˆ˜ ìˆìŒ
@st.cache_data(show_spinner=False, hash_funcs={docx.document.Document: id, pd.DataFrame: pd.util.hash_pandas_object})
def read_file(uploaded_file_content, filename, file_type) -> Tuple[str, Optional[str]]:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ (bytes ë˜ëŠ” buffer) ë°›ì•„ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (ë‚´ìš©, None), ì‹¤íŒ¨ ì‹œ ('', ì—ëŸ¬ ë©”ì‹œì§€) ë°˜í™˜
    """
    try:
        logging.info(f"Reading file content for: {filename} (Type: {file_type})")

        if file_type == 'text/plain':
            try:
                content = uploaded_file_content.decode('utf-8')
            except UnicodeDecodeError:
                logging.warning(f"UTF-8 decoding failed for {filename}, trying cp949.")
                content = uploaded_file_content.decode('cp949')
            return content, None
        elif file_type == 'application/pdf':
            # PdfReaderëŠ” íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ë¥˜(file-like) ê°ì²´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
            # uploaded_file_contentê°€ BytesIO ê°ì²´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
            reader = PdfReader(uploaded_file_content)
            text_parts = []
            for i, page in enumerate(reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)
                except Exception as page_err:
                    logging.warning(f"Error extracting text from page {i+1} of {filename}: {page_err}")
            return '\n'.join(text_parts), None
        elif 'wordprocessingml.document' in file_type:
             # python-docxë„ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ë¥˜ ê°ì²´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
            doc = docx.Document(uploaded_file_content)
            return '\n'.join(p.text for p in doc.paragraphs), None
        elif 'spreadsheetml.sheet' in file_type:
             # pandas read_excelë„ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ë¥˜ ê°ì²´ë¥¼ ë°›ìŠµë‹ˆë‹¤.
            df = pd.read_excel(uploaded_file_content, engine='openpyxl')
            # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ CSV í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            return df.to_csv(index=False, sep='\t'), None
        else:
            logging.warning(f"Unsupported file type for reading: {file_type}")
            return '', f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_type}"
    except Exception as e:
        logging.error(f"Error reading file content for {filename}: {e}", exc_info=True)
        return '', f"íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


@st.cache_data(show_spinner=False)
def load_history(path: str) -> List[Dict[str, str]]:
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        logging.info(f"History file not found at {path}, starting new history.")
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            history = json.load(f)
            logging.info(f"Loaded {len(history)} messages from {path}.")
            # ì„¸ì…˜ ì‹œì‘ ì‹œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì œì™¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # history = [msg for msg in history if msg['role'] != 'system'] # ì´ì „ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë¡œë“œ ë°©ì§€ (ì„ íƒ ì‚¬í•­)
            return history
    except json.JSONDecodeError:
        logging.warning(f"History file {path} is corrupted or invalid. Backing up and starting new history.")
        try:
            # corrupted íŒŒì¼ ë°±ì—… ì‹œì ì„ ì •í™•íˆ í•˜ê¸° ìœ„í•´ pd.Timestamp ëŒ€ì‹  datetime ì‚¬ìš©
            import datetime
            backup_path = f"{path}.{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.bak"
            os.rename(path, backup_path)
            logging.info(f"Corrupted history file backed up to {backup_path}")
            st.sidebar.warning(f"ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì†ìƒë˜ì–´ ë°±ì—… í›„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤: {os.path.basename(backup_path)}")
        except OSError as e:
            logging.error(f"Failed to backup corrupted history file {path}: {e}")
            st.sidebar.error(f"ì†ìƒëœ ê¸°ë¡ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        logging.error(f"Error loading history from {path}: {e}", exc_info=True)
        st.sidebar.error(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def save_history(path: str, msgs: List[Dict[str, str]]):
    """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    msgs_to_save = [msg for msg in msgs if msg['role'] != 'system']
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (Streamlit Cloud / ì¼ë¶€ í™˜ê²½ ëŒ€ë¹„)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(msgs_to_save, f, ensure_ascii=False, indent=2)
        # logging.info(f"Saved {len(msgs_to_save)} messages to {path}.") # ë„ˆë¬´ ìì£¼ ë¡œê¹…ë  ìˆ˜ ìˆìŒ
    except Exception as e:
        # Streamlit Cloudì—ì„œ ì“°ê¸° ê¶Œí•œì´ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
        logging.error(f"Error saving history to {path}: {e}", exc_info=True)
        # st.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # ì‚¬ìš©ìì—ê²Œ ë„ˆë¬´ ìì£¼ ë³´ì¼ ìˆ˜ ìˆìŒ


# ------------------------------------------------------------------
# SESSION STATE INITIALIZATION
# ------------------------------------------------------------------
if 'messages' not in st.session_state:
    # history ë¡œë“œ ì‹œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì œì™¸í•˜ê³  ë¡œë“œ
    st.session_state.messages: List[Dict[str, str]] = load_history(HISTORY_FILE)
    # ë¡œë“œëœ ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´, ì²« ë²ˆì§¸ ë©”ì‹œì§€ê°€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì¸ ê²½ìš° ìŠ¤í‚µí•©ë‹ˆë‹¤.
    # í•˜ì§€ë§Œ load_historyì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•˜ë„ë¡ ìˆ˜ì •í–ˆìœ¼ë¯€ë¡œ ì´ì¤‘ ì²´í¬ëŠ” ë¶ˆí•„ìš”
    # í•„ìš”ì— ë”°ë¼ ì—¬ê¸°ì— í˜„ì¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì²« ë©”ì‹œì§€ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆ: if not st.session_state.messages or st.session_state.messages[0]['role'] != 'system':
    #         st.session_state.messages.insert(0, SYSTEM_PROMPT)

if 'doc_summaries' not in st.session_state:
    st.session_state.doc_summaries: Dict[str, str] = {}
if 'processed_file_ids' not in st.session_state:
    st.session_state.processed_file_ids: set = set()
# íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸°ì—´ ìƒíƒœ ì¶”ê°€
if 'file_to_summarize' not in st.session_state:
    st.session_state.file_to_summarize: Optional[Dict] = None


# ------------------------------------------------------------------
# SIDEBAR: MODEL, MODE SELECTION & OPTIONS
# ------------------------------------------------------------------
st.sidebar.title("âš™ï¸ ì„¤ì •")

MODEL = st.sidebar.selectbox(
    'ëª¨ë¸ ì„ íƒ',
    list(MODEL_CONTEXT_LIMITS.keys()),
    index=list(MODEL_CONTEXT_LIMITS.keys()).index("gpt-4o") if "gpt-4o" in MODEL_CONTEXT_LIMITS else 0 # ê¸°ë³¸ê°’ gpt-4o ì‹œë„
)
MAX_CONTEXT_TOKENS = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

MODE = st.sidebar.radio('ì‘ë‹µ ëª¨ë“œ', ('Poetic', 'Logical'), index=0, key='mode_selection')


st.sidebar.markdown("---")
st.sidebar.subheader("ê´€ë¦¬")

# ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
def build_full_session_content() -> str:
    """ë¬¸ì„œ ìš”ì•½ê³¼ ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•©ì³ í…ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    parts = []
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    parts.append(f"Liel Chat Session Content - {timestamp}")
    parts.append(f"Model: {MODEL}, Mode: {MODE}\n")

    if st.session_state.doc_summaries:
        parts.append("===== Uploaded Document Summaries =====")
        for fname, summ in st.session_state.doc_summaries.items():
            parts.append(f"\n--- Summary: {fname} ---")
            parts.append(summ)
            parts.append("-" * (len(fname) + 16))
        parts.append("\n" + "=" * 30 + "\n")

    parts.append("===== Conversation History =====")
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ë‹¤ìš´ë¡œë“œ ë‚´ìš©ì— í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (save_historyì™€ ì¼ê´€ì„± ìœ ì§€)
    msgs_to_include = [msg for msg in st.session_state.messages if msg['role'] != 'system']
    if not msgs_to_include:
         parts.append("(No conversation yet)")
    else:
        for m in msgs_to_include:
            role_icon = "ğŸ‘¤ User" if m['role'] == 'user' else "ğŸ¤– Liel"
            parts.append(f"\n{role_icon}:\n{m['content']}")
            parts.append("-" * 20)

    return '\n'.join(parts)

# ëŒ€í™”ë‚˜ ë¬¸ì„œ ìš”ì•½(ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)ì´ ìˆì„ ë•Œë§Œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
if [msg for msg in st.session_state.messages if msg['role'] != 'system'] or st.session_state.doc_summaries:
    session_content_txt = build_full_session_content()
    download_filename = f"liel_session_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.txt"
    st.sidebar.download_button(
        label="ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‚´ìš© ë‹¤ìš´ë¡œë“œ", # ë²„íŠ¼ ë ˆì´ë¸” ë³€ê²½
        data=session_content_txt.encode('utf-8'), # UTF-8 ì¸ì½”ë”© ëª…ì‹œ
        file_name=download_filename,
        mime='text/plain',
        help="ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ê³¼ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤." # ë„ì›€ë§ ë³€ê²½
    )

# í•„ìš”í•˜ë‹¤ë©´ ì´ˆê¸°í™” ë²„íŠ¼ì„ ë‹¤ì‹œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
if st.sidebar.button("ğŸ”„ ëŒ€í™” ë° ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.session_state.doc_summaries = {}
    st.session_state.processed_file_ids = set()
    st.session_state.file_to_summarize = None # ì²˜ë¦¬ ëŒ€ê¸° íŒŒì¼ë„ ì´ˆê¸°í™”

    # chat_history.json íŒŒì¼ ì‚­ì œ
    if os.path.exists(HISTORY_FILE):
        try:
            os.remove(HISTORY_FILE)
            logging.info(f"History file {HISTORY_FILE} removed.")
            st.sidebar.success("ëŒ€í™” ê¸°ë¡ íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except OSError as e:
            st.sidebar.error(f"ê¸°ë¡ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            logging.error(f"Failed to remove history file {HISTORY_FILE}: {e}")
    st.rerun()


# ------------------------------------------------------------------
# SYSTEM PROMPT DEFINITION
# ------------------------------------------------------------------
# SYSTEM_PROMPT ì •ì˜ëŠ” ì„¸ì…˜ ì‹œì‘ ì‹œ í•œë²ˆë§Œ í•˜ê±°ë‚˜, ëª¨ë“œê°€ ë°”ë€” ë•Œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.
# Streamlitì€ ë§¤ ì‹¤í–‰ë§ˆë‹¤ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëŒë¯€ë¡œ SYSTEM_PROMPT ì •ì˜ ìì²´ëŠ” ê³„ì† ì¼ì–´ë‚˜ì§€ë§Œ,
# ì´ë¥¼ ë©”ì‹œì§€ ëª©ë¡ì— ì¶”ê°€í•˜ëŠ” ë¡œì§ì€ ì‹ ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤.
SYSTEM_PROMPT_CONTENT = (
    'You are Liel, a poetic, emotionally intelligent chatbot with lyrical grace. Respond with warmth, creativity, and empathy. Use rich language and metaphors when appropriate.'
    if MODE == 'Poetic' else
    'You are Liel, a highly analytical assistant focused on logic and precision. Provide clear, structured, and concise answers. Use bullet points or numbered lists for clarity when needed.'
)
SYSTEM_PROMPT = {'role': 'system', 'content': SYSTEM_PROMPT_CONTENT}

# ì„¸ì…˜ì´ ì‹œì‘ë  ë•Œ (messagesê°€ ì´ˆê¸°í™”ë  ë•Œ) ë˜ëŠ” ëª¨ë“œê°€ ë³€ê²½ë  ë•Œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
# ë§¤ë²ˆ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ë©”ì‹œì§€ ëª©ë¡ì˜ ì²« ìš”ì†Œê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ ,
# ë‹¤ë¥´ê±°ë‚˜ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸/ì¶”ê°€í•©ë‹ˆë‹¤.
if not st.session_state.messages or st.session_state.messages[0]['role'] != 'system' or st.session_state.messages[0]['content'] != SYSTEM_PROMPT_CONTENT:
     # ê¸°ì¡´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±°
     st.session_state.messages = [msg for msg in st.session_state.messages if msg['role'] != 'system']
     # ìƒˆ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
     st.session_state.messages.insert(0, SYSTEM_PROMPT)
     # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë³€ê²½ ì‹œ history.jsonì— ì €ì¥ë˜ì§€ ì•Šë„ë¡ save_history í•¨ìˆ˜ì—ì„œ ì œì™¸ ì²˜ë¦¬ í•„ìš” (ì´ë¯¸ êµ¬í˜„ë¨)


# ------------------------------------------------------------------
# UI HEADER
# ------------------------------------------------------------------
st.title(f'ğŸ’¬ Liel â€“ {MODE} Chatbot')
st.caption(
    "ê¸°ì–µê³¼ ê°ì •ìœ¼ë¡œ ë¹›ë‚˜ëŠ” ë‹¹ì‹ ì˜ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if MODE == 'Poetic' else
    "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸ ëŒ€í™” ìƒëŒ€, Lielì…ë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
)

# ------------------------------------------------------------------
# FILE UPLOAD UI
# ------------------------------------------------------------------
uploaded_file = st.file_uploader(
    'íŒŒì¼ ì—…ë¡œë“œ (txt, pdf, docx, xlsx)',
    type=['txt', 'pdf', 'docx', 'xlsx'],
    key="file_uploader",
    help="í…ìŠ¤íŠ¸, PDF, ì›Œë“œ, ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•©ë‹ˆë‹¤."
)

# --- íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ë¡œì§ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©) ---
# íŒŒì¼ ì—…ë¡œë” ìœ„ì ¯ì— ìƒˆë¡œìš´ íŒŒì¼ ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
if uploaded_file is not None:
    # íŒŒì¼ ê°ì²´ì˜ ê³ ìœ  ID ì‚¬ìš© ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ íŒŒì¼ ì‹ë³„ (ì˜ˆ: ì´ë¦„, í¬ê¸° ì¡°í•©)
    # st.rerun() ë“±ìœ¼ë¡œ ì¸í•´ uploaded_file ê°ì²´ ìì²´ê°€ Noneì´ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ IDë¥¼ ë¨¼ì € í™•ë³´
    current_uploaded_file_id = uploaded_file.id # Streamlit 1.29.0 ì´ìƒì—ì„œ ì§€ì›

    # ì´ì „ì— ì²˜ë¦¬í–ˆê±°ë‚˜ í˜„ì¬ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ì¸ì§€ í™•ì¸
    # file_to_summarize ìƒíƒœë¥¼ ë¨¼ì € í™•ì¸í•˜ì—¬ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
    if 'file_to_summarize' not in st.session_state or \
       st.session_state.file_to_summarize is None or \
       st.session_state.file_to_summarize['id'] != current_uploaded_file_id: # í˜„ì¬ íŒŒì¼ê³¼ ë‹¤ë¥¸ ê²½ìš° ìƒˆë¡œ ì²˜ë¦¬

         if current_uploaded_file_id not in st.session_state.processed_file_ids:
            logging.info(f"New file detected: {uploaded_file.name} (ID: {current_uploaded_file_id})")
            # íŒŒì¼ ë‚´ìš©ì„ ì½ì–´ì„œ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ê³ , ì²˜ë¦¬ëŠ” ë‹¤ìŒ Streamlit ì‹¤í–‰ ì£¼ê¸°ì— ì§„í–‰
            # uploaded_file.getvalue()ëŠ” íŒŒì¼ ê°ì²´ ìì²´ ë˜ëŠ” ê·¸ ë‚´ìš©ì„ ë°˜í™˜
            file_content_bytes = uploaded_file.getvalue()
            filename_to_process = uploaded_file.name
            filetype_to_process = uploaded_file.type

            # íŒŒì¼ ë‚´ìš©ì„ ì½ê³  ì—ëŸ¬ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬ ëŒ€ê¸° ìƒíƒœë¡œ ì €ì¥
            # read_file í•¨ìˆ˜ëŠ” íŒŒì¼ë¥˜ ê°ì²´ë¥¼ ë°›ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
            import io
            content_text, read_error = read_file(io.BytesIO(file_content_bytes), filename_to_process, filetype_to_process)


            if read_error:
                st.error(f"'{filename_to_process}' íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {read_error}")
                # ì‹¤íŒ¨í•œ íŒŒì¼ë„ processed_file_idsì— ì¶”ê°€í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒ ì‚¬í•­)
                # st.session_state.processed_file_ids.add(current_uploaded_file_id)
            elif not content_text:
                st.warning(f"'{filename_to_process}' íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                st.session_state.processed_file_ids.add(current_uploaded_file_id) # ë¹ˆ íŒŒì¼ë„ ì²˜ë¦¬ ì™„ë£Œë¡œ í‘œì‹œ
            else:
                # ì²˜ë¦¬í•  íŒŒì¼ ì •ë³´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.file_to_summarize = {
                    'id': current_uploaded_file_id,
                    'name': filename_to_process,
                    'content': content_text # í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì €ì¥
                }
                logging.info(f"File '{filename_to_process}' stored in session state for processing.")
                # íŒŒì¼ ì—…ë¡œë“œ ê°ì§€ í›„ ë°”ë¡œ Rerunì„ í˜¸ì¶œí•˜ì—¬ íŒŒì¼ ì²˜ë¦¬ ë¡œì§ì´ ì‹œì‘ë˜ë„ë¡ í•©ë‹ˆë‹¤.
                st.rerun() # Streamlit ì¬ì‹¤í–‰ (íŒŒì¼ ì²˜ë¦¬ ë¡œì§ìœ¼ë¡œ ì´ë™)


# --- íŒŒì¼ ì²˜ë¦¬ ë° ìš”ì•½ ë¡œì§ (ì„¸ì…˜ ìƒíƒœì—ì„œ íŒŒì¼ ì •ë³´ë¥¼ ì½ì–´ì˜´) ---
# ì„¸ì…˜ ìƒíƒœì— ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ì¸ íŒŒì¼ì´ ìˆê³ , ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°
if 'file_to_summarize' in st.session_state and \
   st.session_state.file_to_summarize is not None and \
   st.session_state.file_to_summarize['id'] not in st.session_state.processed_file_ids:

    file_info = st.session_state.file_to_summarize
    file_id_to_process = file_info['id']
    filename_to_process = file_info['name']
    file_content_to_process = file_info['content']

    # ì²˜ë¦¬ ëŒ€ê¸°ì—´ì—ì„œ íŒŒì¼ ì •ë³´ ì œê±°
    st.session_state.file_to_summarize = None

    logging.info(f"Starting processing file from session state: {filename_to_process} (ID: {file_id_to_process})")

    with st.spinner(f"'{filename_to_process}' ì²˜ë¦¬ ë° ìš”ì•½ ì¤‘..."):
        tokenizer = get_tokenizer()
        # summarize_document í•¨ìˆ˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°”ë¡œ ë°›ë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        summary, summary_error = summarize_document(file_content_to_process, filename_to_process, MODEL, tokenizer) # summarize_document contains API calls and progress bar

        if summary_error:
             st.warning(f"'{filename_to_process}' ìš”ì•½ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ:\n{summary_error}")

        st.session_state.doc_summaries[filename_to_process] = summary
        st.session_state.processed_file_ids.add(file_id_to_process) # ì²˜ë¦¬ ì™„ë£Œ ID ì¶”ê°€

    st.success(f"ğŸ“„ '{filename_to_process}' ì—…ë¡œë“œ ë° ìš”ì•½ ì™„ë£Œ!")
    logging.info(f"Successfully processed and summarized file: {filename_to_process}")
    # ìš”ì•½ ì™„ë£Œ í›„ Rerunì„ í˜¸ì¶œí•˜ì—¬ UI (ì˜ˆ: ìš”ì•½ Expander)ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    st.rerun()


# ìš”ì•½ëœ ë¬¸ì„œ í‘œì‹œ (Expander ì‚¬ìš©)
if st.session_state.doc_summaries:
    with st.expander("ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ìš”ì•½ ë³´ê¸°", expanded=False):
        for fname, summ in st.session_state.doc_summaries.items():
            st.text_area(f"ìš”ì•½: {fname}", summ, height=150, key=f"summary_display_{fname}", disabled=True)
        # í•„ìš”í•˜ë‹¤ë©´ ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ëŠ” ë²„íŠ¼ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if st.button("ë¬¸ì„œ ìš”ì•½ë§Œ ì§€ìš°ê¸°", key="clear_doc_summaries_btn_exp"):
             st.session_state.doc_summaries = {}
             st.session_state.processed_file_ids = set() # ë¬¸ì„œ ìš”ì•½ ê´€ë ¨ IDë§Œ ì§€ìš°ê±°ë‚˜ ë³„ë„ ê´€ë¦¬ í•„ìš” ì‹œ ìˆ˜ì •
             logging.info("Document summaries cleared by user from expander button.")
             st.rerun()


# ------------------------------------------------------------------
# DISPLAY CHAT HISTORY
# ------------------------------------------------------------------
st.markdown("---")
st.subheader("ëŒ€í™”")

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ëŒ€í™” ëª©ë¡ì— í‘œì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
msgs_to_display = [msg for msg in st.session_state.messages if msg['role'] != 'system']

for message in msgs_to_display:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ------------------------------------------------------------------
# CHAT INPUT & RESPONSE GENERATION (with Streaming)
# ------------------------------------------------------------------
if prompt := st.chat_input("ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ê¸°ë¡ (session_stateì— ì¶”ê°€)
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    st.session_state.messages.append({'role': 'user', 'content': prompt})

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)

    # --- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ---
    try:
        tokenizer = get_tokenizer()
        current_model_max_tokens = MODEL_CONTEXT_LIMITS.get(MODEL, 4096)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # st.session_state.messagesì˜ ì²« ìš”ì†Œê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
        # (SYSTEM_PROMPT ì •ì˜ ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë¡œì§ì— ë”°ë¼)
        current_system_prompt = st.session_state.messages[0] if st.session_state.messages and st.session_state.messages[0]['role'] == 'system' else SYSTEM_PROMPT

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì˜ˆì•½ í† í°ì„ ì œì™¸í•œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í† í°
        base_tokens = num_tokens_from_messages([current_system_prompt], tokenizer)
        available_tokens_for_context = current_model_max_tokens - base_tokens - RESERVED_TOKENS

        if available_tokens_for_context <= 0:
             st.error("ì„¤ì •ëœ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì˜ˆì•½ í† í°ìœ¼ë¡œ ì¸í•´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ì˜ˆì•½ í† í° ê°ì†Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
             logging.error("Not enough tokens for context construction after system prompt and reserved tokens.")
             # st.stop() # ì•± ì „ì²´ ì¤‘ì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ í‘œì‹œ
             raise ValueError("Context window too small") # ì˜¤ë¥˜ ë°œìƒì‹œì¼œ í•˜ìœ„ ë¡œì§ ì¤‘ë‹¨


        # ì‹¤ì œ API í˜¸ì¶œì— ì‚¬ìš©ë  ë©”ì‹œì§€ ëª©ë¡ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
        conversation_context = [current_system_prompt]
        tokens_used = base_tokens

        # ë¬¸ì„œ ìš”ì•½ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        doc_summary_context = []
        doc_tokens_added = 0
        for fname, summ in reversed(list(st.session_state.doc_summaries.items())):
            summary_msg = {'role': 'system', 'content': f"[ë¬¸ì„œ '{fname}' ìš”ì•½ ì°¸ê³ ]\n{summ}"}
            temp_context = [summary_msg]
            summary_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš© + ì¶”ê°€ë  ìš”ì•½)
            if available_tokens_for_context - (tokens_used - base_tokens + doc_tokens_added + summary_tokens) >= 0:
                doc_summary_context.insert(0, summary_msg)
                doc_tokens_added += summary_tokens
            else:
                logging.warning(f"Document summary '{fname}' skipped due to token limit.")
                break

        conversation_context.extend(doc_summary_context)
        tokens_used += doc_tokens_added


        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (í† í° ì˜ˆì‚° ë‚´ì—ì„œ ìµœì‹ ìˆœ)
        history_context = []
        history_tokens_added = 0
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ëŒ€í™” ê¸°ë¡ë§Œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        msgs_to_consider = [msg for msg in st.session_state.messages if msg['role'] != 'system']

        for msg in reversed(msgs_to_consider):
            temp_context = [msg]
            msg_tokens = num_tokens_from_messages(temp_context, tokenizer)

            # ì‚¬ìš© ê°€ëŠ¥í•œ í† í° = ì „ì²´ ê°€ìš© - (í˜„ì¬ ì‚¬ìš©(ìš”ì•½í¬í•¨) - base + ì¶”ê°€ë  íˆìŠ¤í† ë¦¬)
            if available_tokens_for_context - ((tokens_used - base_tokens) + history_tokens_added + msg_tokens) >= 0:
                history_context.insert(0, msg)
                history_tokens_added += msg_tokens
            else:
                logging.warning("Older chat history skipped due to token limit.")
                break

        conversation_context.extend(history_context)
        tokens_used += history_tokens_added


        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ ë¡œê¹…
        logging.info(f"Context constructed with {tokens_used} tokens for model {MODEL}.")
        # logging.debug(f"Final conversation context: {conversation_context}") # í•„ìš”ì‹œ ìƒì„¸ ë¡œê¹…

    except Exception as e:
        st.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Error constructing conversation context: {e}", exc_info=True)
        # st.stop() # ì•± ì „ì²´ ì¤‘ì§€ ëŒ€ì‹  ì˜¤ë¥˜ ë©”ì‹œì§€ë§Œ í‘œì‹œ
        conversation_context = [current_system_prompt, {'role': 'user', 'content': prompt}] # ìµœì†Œí•œì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¬ì‹œë„ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ë§Œ

    # --- API í˜¸ì¶œ ë° ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ---
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ê±°ë‚˜, ì˜¤ë¥˜ ì²˜ë¦¬ í›„ ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì§„í–‰
    if conversation_context and (len(conversation_context) > 1 or conversation_context[0]['role'] == 'system'):
        with st.chat_message("assistant"):
            message_placeholder = st.empty() # ì‘ë‹µ í‘œì‹œ ì˜ì—­
            full_response = ""
            try:
                stream = client.chat.completions.create(
                    model=MODEL,
                    messages=conversation_context,
                    stream=True,
                    temperature=0.75 if MODE == 'Poetic' else 0.4, # ëª¨ë“œë³„ ì˜¨ë„ ì¡°ì ˆ
                    # max_tokens= # í•„ìš”ì‹œ ìµœëŒ€ ì‘ë‹µ ê¸¸ì´ ì„¤ì • ê°€ëŠ¥
                    timeout=120 # ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ (ì´ˆ)
                )
                # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° í‘œì‹œ
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        chunk_content = chunk.choices[0].delta.content
                        full_response += chunk_content
                        message_placeholder.markdown(full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼

                message_placeholder.markdown(full_response) # ìµœì¢… ì‘ë‹µ í‘œì‹œ
                logging.info(f"Assistant response received (length: {len(full_response)} chars).")

            except Exception as e:
                full_response = f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                message_placeholder.error(full_response)
                logging.error(f"Error during OpenAI API call or streaming: {e}", exc_info=True)

    else:
         full_response = "âš ï¸ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨ë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
         st.chat_message("assistant").error(full_response)


    # ì‘ë‹µ ê¸°ë¡ ì €ì¥ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)
    if full_response and full_response.startswith("âš ï¸"): # API ì˜¤ë¥˜ ë©”ì‹œì§€ë„ ì €ì¥
        st.session_state.messages.append({'role': 'assistant', 'content': full_response})
        save_history(HISTORY_FILE, st.session_state.messages)
    elif full_response: # ì •ìƒ ì‘ë‹µ
         st.session_state.messages.append({'role': 'assistant', 'content': full_response})
         save_history(HISTORY_FILE, st.session_state.messages)


# --- Footer or additional info ---
st.sidebar.markdown("---")
st.sidebar.caption("Liel Chatbot v1.2")